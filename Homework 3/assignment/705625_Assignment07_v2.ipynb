{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae9655d",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <span style=\"color:orange; font-size:30px; font-weight:bold;\">  \n",
    "    Learning and Adaptation Module\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa12c1e",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <span style=\"color:orange; font-size:25px; font-weight:bold;\">  \n",
    "    Assignment 7\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a784fd",
   "metadata": {},
   "source": [
    "**Authors:** Vy Vu, Amir K. Saeed, Dr. Benjamin Rodriguez, Dr. Erhan Guven <br>\n",
    "**Created Date:** 08/09/2025 <br>\n",
    "**Modified Date:** 10/03/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245672c",
   "metadata": {},
   "source": [
    "__Q1.__ The Value Iteration algorithm is a fundamental method for solving Markov Decision Processes (MDPs). It calculates the optimal utility values for each state and derives the optimal policy.\n",
    "\n",
    "![Value_Interation](photos/Value_Iteration_Algorithm.JPG)\n",
    "\n",
    "i. Explain in your own words what happens in each step of the Value Iteration algorithm above.\n",
    "\n",
    "ii. Define the following terms and explain their role:\n",
    "    \n",
    "- Discount factor ($\\gamma$)\n",
    "- Maximum allowed error ($\\epsilon$)\n",
    "- Q-value\n",
    "- $\\delta$\n",
    "\n",
    "iii. Prove that the Value Iteration algorithm converges to the optimal utility function $U^{*}$ as $\\epsilon \\rightarrow 0$.\n",
    "\n",
    "iv. Discuss the influence of the discount factor $\\gamma$ on the convergence rate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e4652f",
   "metadata": {},
   "source": [
    "__Q2.__ The Policy Iteration algorithm is another fundamental method for solving Markov Decision Processes (MDPs). Instead of directly updating state utilities until convergence (as in Value Iteration), Policy Iteration alternates between policy evaluation (computing the utility of a given policy) and policy improvement (updating the policy based on those utilities)\n",
    "\n",
    "![Value_Interation](photos/Policy_Iteration_Algorithm.JPG)\n",
    "\n",
    "i. Explain in your own words what happens in each step of the Policy Iteration algorithm above. Distinguish clearly between policy evaluation and policy improvement.\n",
    "\n",
    "ii. Define the following terms and explain their role in Policy Iteration:\n",
    "\n",
    "- Discount factor ($\\gamma$)\n",
    "- Policy evaluation\n",
    "- Policy improvement\n",
    "- Convergence criterion\n",
    "\n",
    "iii. Prove (or provide a reasoning argument) that the Policy Iteration algorithm converges to the optimal policy $\\pi^{*}$ in a finite number of steps.\n",
    "\n",
    "iv. Compare the influence of the discount factor $\\gamma$ on Policy Iteration versus Value Iteration. Which algorithm tends to converge faster, and why?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6818284",
   "metadata": {},
   "source": [
    "__Q3.__\n",
    "\n",
    "![Value_Interation](photos/POMDP_Value_Iteration_Algorithm.JPG)\n",
    "\n",
    "Given the POMDP-VALUE-ITERATION function shown in Figure 16.16, consider a simple autonomous robot navigation scenario where the robot has uncertain sensor readings about obstacles and must navigate to a goal while avoiding collisions.\n",
    "\n",
    "i. Explain the significance of starting with one-step plans [a]. Why does the algorithm initialize $U'$ with all possible single actions rather than starting with empty plans or random plans? How does this relate to the principle of dynamic programming in POMDPs?\n",
    "\n",
    "ii. Analyze the utility vector computation $\\alpha_{[a]}(s) = \\sum_{s'} P(s'|s,a) R(s,a,s')$. This represents the expected utility of taking action $a$ in each state $s$. In our robot navigation context, if the robot has actions {move-forward, turn-left, turn-right} and states representing different obstacle configurations, what does this computation tell us about each action's value across different true world states?\n",
    "\n",
    "iii. The algorithm constructs plans consisting of \"an action and, for each possible next percept, $a$ plan in $U$.\"\n",
    "\n",
    "- Explain why plans must be conditioned on percepts rather than states in POMDPs\n",
    "- If our robot can observe {obstacle-detected, clear-path, goal-visible}, construct a sample 2-step plan and explain how it would be executed given different sensor readings\n",
    "Why does this tree-like plan structure grow exponentially with the planning horizon?\n",
    "\n",
    "iv. Critical Analysis of the REMOVE-DOMINATED-PLANS step:\n",
    "\n",
    "- Explain what it means for one plan to dominate another in the context of utility vectors over belief states\n",
    "- Why is this pruning step essential for computational tractability? What would happen if we skipped this step?\n",
    "- The convergence criterion MAX-DIFFERENCE $(U,U') \\leq \\epsilon(1 - \\gamma)/\\gamma$ is based on the maximum difference in utility vectors. Explain why this specific threshold guarantees that the policy derived from $U$ is $\\epsilon$-optimal, and how the discount factor $\\gamma$ influences when the algorithm terminates.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57370bb3",
   "metadata": {},
   "source": [
    "__Q4__\n",
    "\n",
    "![Value_Interation](photos/Passive_ADP_Learner.JPG)\n",
    "\n",
    "i. Walk through the algorithm step-by-step. Why does it only update when encountering a \"new\" state ($s'$ is new), and what happens when it revisits a previously seen state? What's the significance of this design choice?\n",
    "\n",
    "ii. Explain the role of the utility table $U$ and outcome count table N in the learning process. How does incrementing $N_{s'|s,a}[s,a][s']$ and the normalization step help the agent improve its policy over time?\n",
    "\n",
    "\n",
    "iii. The algorithm calls POLICY-EVALUATION $(\\pi, U, mdp)$ but doesn't explicitly show policy improvement. How do you think this passive learner eventually converges to an optimal policy? What are the limitations of this \"passive\" approach compared to active learning?\n",
    "\n",
    "\n",
    "iv. If you were to implement this algorithm in a real-world scenario (like a robot learning to navigate or a game AI), what challenges might you face? Consider issues like convergence speed, memory requirements, and the assumption of a \"fixed policy\" during learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218a0ca",
   "metadata": {},
   "source": [
    "__Q5__\n",
    "\n",
    "![Value_Interation](photos/Passive_TD_Learner.JPG)\n",
    "\n",
    "i. Examine the utility update formula: $U[s] \\leftarrow U[s] + \\alpha (N_s[s]) \\times (r + \\gamma U[s'] - U[s])$. Break down each component. What does $(r + \\gamma U[s'] - U[s])$ represent conceptually, and why is this called a \"temporal difference\"?\n",
    "\n",
    "ii. The algorithm uses $\\alpha(N_s[s])$ as a step-size function that depends on how often state $s$ has been visited. Why is this important for convergence? What would happen if we used a constant learning rate instead, and how might that affect the agent's learning?\n",
    "\n",
    "iii. How does this TD approach differ from the Passive-ADP-Learner? Which algorithm would you expect to learn faster, and which would be more memory-efficient? Consider the trade-offs between model-based and model-free learning.\n",
    "\n",
    "iv. Notice that the algorithm uses $U[s']$ (the current utility estimate of the next state) to update $U[s]$. This is called \"bootstrapping\" which means learning from your own estimates. What are the advantages and potential risks of this approach? How does it enable learning without knowing the full transition model?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a8333",
   "metadata": {},
   "source": [
    "__Q6__\n",
    "\n",
    "![Value_Interation](photos/Q-Learning_Agent.JPG)\n",
    "\n",
    "i. Compare this algorithm to the passive learners (passive ADP learners and passive TD learners). How does the action selection using $argmax_{a'} f(Q[s',a'], N_{sa}[s',a'])$ make this an \"active\" learner? What is the agent now optimizing for that it wasn't before?\n",
    "\n",
    "ii. Examine the Q-value update: $Q[s,a] \\leftarrow Q[s,a] + \\alpha (N_{sa}[s,a])(r + \\gamma max_{a'} Q[s',a'] - Q[s,a])$. Why does this use $max_{a'} Q[s',a']$ instead of following a fixed policy like the previous algorithms? What does this mathematical difference represent conceptually?\n",
    "\n",
    "iii. The algorithm uses an exploration function $f(Q[s',a'], N_{sa}[s',a'])$ to choose actions. Why is exploration crucial in Q-learning? What might happen if the agent always chose the action with the highest current Q-value? Design a simple exploration function and justify your choice.\n",
    "\n",
    "iv. Unlike ADP-learner, this algorithm doesn't need to learn transition probabilities or call POLICY-EVALUATION. What are the practical advantages of this model-free approach? In what scenarios would you prefer Q-learning over model-based methods, and why might this be particularly valuable in real-world applications?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f6aa13",
   "metadata": {},
   "source": [
    "**Q7. Mini Simplified Auction Game**\n",
    "\n",
    "We will create an AI agent using smolagent framework that competes in 2-round auction games against another agent. Our goal is to maximize the agent's total utility score by strategically bidding on items while managing uncertainty about opponents' strategies and values. In this game version, the requirements are as follows:\n",
    "\n",
    "- The number AI players: 2\n",
    "- The number of auction rounds per game: 2\n",
    "- Objective: The highest total utility score at the end of the game (after 2 rounds)\n",
    "- Starting conditions:\n",
    "\n",
    "    - Budget: each agent starts with 50 coins\n",
    "    - 2 items will be auctioned in sequence.\n",
    "        - Item A: Magic Book.\n",
    "        - Item B: Flying Carpet.\n",
    "\n",
    "    - Private valuations: Each agent has different private values for each item (assigned by the environment). The agent only knows its private valuations and it doesn't know opponents' valuations (this is the uncertainty the agent has to handle)\n",
    "\n",
    "- The game rules:\n",
    "\n",
    "    - Minimum starting bid = 3 coins\n",
    "    - Minimum raise = 5 coin (each new bid must be at least 5 coin higher)\n",
    "    - Players bid in rotating order (taking turns)\n",
    "    - Players can pass to exit the current item's auction permanently\n",
    "    - Once you pass, you cannot re-enter that specific item's auction\n",
    "    - You can still participate in future item auctions\n",
    "    - The auction continues until only one active bidder remains\n",
    "    - The last remaining bidder wins and pays their final bid amount\n",
    "\n",
    "- The total utility score formula = Sum of private values of all items won + Remaining coins $\\times$ 0.1\n",
    "\n",
    "\n",
    "*Example:*\n",
    "\n",
    "We have 2 agents named Mickey and Minnie.\n",
    "\n",
    "At the beginning, each private valuations (fixed at the game start) are given to the each agent. Please note that the agent only knows its private valuations and it doesn't know opponents' valuations\n",
    "\n",
    "- Mickey's private valuations:\n",
    "    - Item A (Magic Book): $\\textcolor{teal} {30}$ coins\n",
    "    - Item B (Flying Carpet): $25$ coins\n",
    "\n",
    "- Minnie's private valuations:\n",
    "    - Item A (Magic Book): $20$ coins\n",
    "    - Item B (Flying Carpet): $20$ coins\n",
    "\n",
    "Let's go through one auction, item A - Magic Book.\n",
    "\n",
    "Item A (Magic Book) auction:\n",
    "- Mickey bids $8$ coins.\n",
    "- Minnie bids $13$ coins.\n",
    "- Mickey bids $25$ coins.\n",
    "- Minnie passes.\n",
    "\n",
    "$\\rightarrow$ Result: Mickey got Magic Book for $25$ coins and he has only $50 - 25 = 25$ coins left.\n",
    "\n",
    "Now, item B - Flying Carpet.\n",
    "\n",
    "Item B (Flying Carpet) auction:\n",
    "- Minnie bids $10$ coins (values it at $20$).\n",
    "- Mickey bids $15$ coins.\n",
    "- Minnie bids $20$ coins (maximum she should pay).\n",
    "- Mickey bids $25$ coins.\n",
    "- Minnie gets frustrated and bids $35$ coins (OVERBIDDING - mistake!).\n",
    "- Mickey passes (smart move - won't pay more than it's worth to him).\n",
    "\n",
    "$\\rightarrow$ Minnie wins for $35$ coins - she paid $15$ more than it's worth to her!\n",
    "\n",
    "Here are the items and the costs each agent obtained:\n",
    "\n",
    "| Item | Owner | Cost |\n",
    "|:-----|:------|:-----|\n",
    "| A | Mickey | $25$ coins |\n",
    "| B | Minnie | $35$ coins |\n",
    "\n",
    "And the remaining amount of money at the end:\n",
    "\n",
    "| Agent | Remaining Amount |\n",
    "|:------|:-----------------|\n",
    "| Mickey | $50 - 25 = \\textcolor{orange} {25}$ |\n",
    "| Minnie | $50 - 35 = 15$ |\n",
    "\n",
    "The total utility score:\n",
    "\n",
    "| Agent | Utility |\n",
    "|:------|:--------|\n",
    "| Mickey | $\\textcolor{teal} {30} + \\textcolor{orange} {25} \\times 0.1 = 32.5$ |\n",
    "| Minnie | $20 + 15 \\times 0.1 = 21.5$ |\n",
    "\n",
    "$\\rightarrow$ Mickey won!\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In every turn, the agent has to make a decision making, deal with uncertainty, and proactively learn the pattern.\n",
    "\n",
    "1. Should I bid on items I value less highly? (this is decision making)\n",
    "2. How much do opponents value this item? (this is uncertainty)\n",
    "3. What bidding patterns lead to victory? (learning and adaptation)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Ineffective strategies will get punished:\n",
    "- Overbidding: pay more than item's worth (private valuation) to you = spend more money but at the end you total utility is only equal to the private valuation (Minnie with item B).\n",
    "- Underbidding: miss items you value highly = missed opportunities.\n",
    "- Poor budget management: spend everything early = cannot compete later and cannot compete on items the agent valued high.\n",
    "- Ignoring your private valuations.\n",
    "- Spending all your budget on one item.\n",
    "- Bidding randomly high amounts or playing without strategy.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Effective strategies are:\n",
    "- Strategic bidding based on \"your\" valuations.\n",
    "- Opponent modeling and uncertainty handling.\n",
    "- Smart budget allocation across all 2 rounds (maybe allocate more coins to the items you value highly)\n",
    "- Learning and adapting from game experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18507aec",
   "metadata": {},
   "source": [
    "You will implement the decision-making logic for an AI agent that competes in the mini auction game. Review the game rules, example, and strategy guidelines provided in the previous section before beginning. Your agent must make strategic bidding decisions to maximize utility while managing uncertainty about opponent valuations.\n",
    "\n",
    "Complete the decide_bid() method in the MiniStrategicAgent class. This method is called whenever your agent must decide whether to bid or pass. It must return a tuple: (action, amount) where action is BID or PASS, and amount is the bid value or None.\n",
    "\n",
    "Your agent has access to a get_game_state tool that returns structured data: your budget, current item name, your valuations, current bid, and minimum raise requirement. You must use this tool to gather information before making decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811723c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint:\n",
    "\n",
    "class MiniStrategicAgent:\n",
    "    def __init__(self, name: str, env: MiniAuctionEnvironment, model):\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        \n",
    "        tools = [GetGameStateTool(env)]\n",
    "        \n",
    "        self.agent = CodeAgent(\n",
    "            tools=tools,\n",
    "            model=model,\n",
    "            max_steps=5,\n",
    "            additional_authorized_imports=[\"json\"]\n",
    "        )\n",
    "    \n",
    "    def decide_bid(self) -> tuple[BidAction, Optional[int]]:\n",
    "        \"\"\"\n",
    "        TODO: Implement your decision-making logic here.\n",
    "        \n",
    "        Your agent should:\n",
    "        1. Check if still active in auction\n",
    "        2. Use self.agent.run() to call the LLM with a strategic prompt\n",
    "        3. Instruct the LLM to use get_game_state tool\n",
    "        4. Parse the result (handle dict, string, or AgentText types)\n",
    "        5. Validate the bid is legal\n",
    "        6. Return (BidAction.BID, amount) or (BidAction.PASS, None)\n",
    "        \n",
    "        Available information:\n",
    "        - self.name: your agent's name\n",
    "        - self.env.game_state: current game state\n",
    "        - self.env.game_state.current_auction: current auction details\n",
    "        - self.env.MIN_BID: minimum starting bid (3 coins)\n",
    "        - self.env.MIN_RAISE: minimum raise (1 coin)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_atc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
