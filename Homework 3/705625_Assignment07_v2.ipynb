{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae9655d",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <span style=\"color:orange; font-size:30px; font-weight:bold;\">  \n",
    "    Learning and Adaptation Module\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa12c1e",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <span style=\"color:orange; font-size:25px; font-weight:bold;\">  \n",
    "    Assignment 7\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a784fd",
   "metadata": {},
   "source": [
    "**Authors:** Vy Vu, Amir K. Saeed, Dr. Benjamin Rodriguez, Dr. Erhan Guven <br>\n",
    "**Created Date:** 08/09/2025 <br>\n",
    "**Modified Date:** 10/03/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245672c",
   "metadata": {},
   "source": [
    "__Q1.__ The Value Iteration algorithm is a fundamental method for solving Markov Decision Processes (MDPs). It calculates the optimal utility values for each state and derives the optimal policy.\n",
    "\n",
    "![Value_Interation](photos/Value_Iteration_Algorithm.jpg)\n",
    "\n",
    "i. Explain in your own words what happens in each step of the Value Iteration algorithm above.\n",
    "\n",
    "ii. Define the following terms and explain their role:\n",
    "    \n",
    "- Discount factor ($\\gamma$)\n",
    "- Maximum allowed error ($\\epsilon$)\n",
    "- Q-value\n",
    "- $\\delta$\n",
    "\n",
    "iii. Prove that the Value Iteration algorithm converges to the optimal utility function $U^{*}$ as $\\epsilon \\rightarrow 0$.\n",
    "\n",
    "iv. Discuss the influence of the discount factor $\\gamma$ on the convergence rate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c810a0",
   "metadata": {},
   "source": [
    "## Q1. Value Iteration for Markov Decision Processes\n",
    "\n",
    "### i) Intuitive explanation of the algorithm\n",
    "\n",
    "Value Iteration works by repeatedly improving our estimate of how good each state is, assuming the agent will always behave optimally in the future. We start with an initial guess of the utilities for all states, usually setting them to zero, which simply means we initially have no information about how valuable any state is.\n",
    "\n",
    "At each iteration, the algorithm updates the utility of every state by looking one step ahead. For a given state, it considers all possible actions, evaluates what reward the agent would receive immediately, and then adds the discounted value of the future states that could result from that action. Among all actions, only the best one is kept, since we are interested in optimal behavior. This step is essentially asking: *“If I were in this state and acted optimally from now on, how good would this state be?”*\n",
    "\n",
    "After updating all states, the algorithm checks how much the utilities have changed compared to the previous iteration. If the changes are still large, it means the estimates are not stable yet, so the process repeats. Once the changes become sufficiently small, the algorithm stops and returns the current utilities as an approximation of the optimal utility function.\n",
    "\n",
    "---\n",
    "\n",
    "### ii) Key concepts and their roles\n",
    "\n",
    "The discount factor \\(\\gamma\\) determines how much future rewards matter compared to immediate rewards. A smaller \\(\\gamma\\) makes the agent more short-sighted, focusing mostly on near-term rewards, while a larger \\(\\gamma\\) encourages long-term planning. Importantly, discounting also ensures that the infinite sum of future rewards stays finite and that the algorithm converges.\n",
    "\n",
    "The maximum allowed error \\(\\epsilon\\) controls how accurate the final utilities need to be. Rather than iterating forever, value iteration stops once the utility values are guaranteed to be within \\(\\epsilon\\) of the optimal utilities. Smaller values of \\(\\epsilon\\) lead to more accurate results but require more iterations.\n",
    "\n",
    "A Q-value represents the expected return of taking a specific action in a given state, assuming optimal behavior afterward. It combines the immediate reward of the action with the discounted utility of possible next states. Value iteration uses Q-values to decide which action gives the highest expected return when updating a state’s utility.\n",
    "\n",
    "The variable \\(\\delta\\) tracks the maximum change in utility for any state during a single iteration. It acts as a practical measure of convergence: if even the largest change is very small, then all utilities are essentially stable.\n",
    "\n",
    "---\n",
    "\n",
    "### iii) Why Value Iteration converges to the optimal utility\n",
    "\n",
    "The reason value iteration converges lies in how the update rule behaves. Each iteration applies the Bellman optimality update, which improves the utility estimates by combining immediate rewards with discounted future utilities. Because future rewards are multiplied by the discount factor \\(\\gamma < 1\\), any errors in the utility estimates shrink over time rather than growing.\n",
    "\n",
    "This update process has a contraction property: each iteration brings the utility function closer to a single fixed point, which is the optimal utility function \\(U^*\\). No matter where we start, repeatedly applying the update rule steadily reduces the distance to this optimal solution. As the stopping tolerance \\(\\epsilon\\) approaches zero, the algorithm is forced to run longer, and the returned utility values converge exactly to \\(U^*\\).\n",
    "\n",
    "---\n",
    "\n",
    "### iv) Influence of the discount factor on convergence rate\n",
    "\n",
    "The discount factor has a strong impact on how fast value iteration converges. When \\(\\gamma\\) is small, future rewards have less influence, so errors in utility estimates disappear quickly and convergence is fast. When \\(\\gamma\\) is close to one, future rewards matter a lot, which means errors propagate further into the future and decay more slowly.\n",
    "\n",
    "In addition, a large \\(\\gamma\\) makes the stopping condition stricter, requiring very small changes in utilities before the algorithm terminates. As a result, value iteration typically converges much more slowly when the discount factor is high, even though the final policy may be more far-sighted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e4652f",
   "metadata": {},
   "source": [
    "__Q2.__ The Policy Iteration algorithm is another fundamental method for solving Markov Decision Processes (MDPs). Instead of directly updating state utilities until convergence (as in Value Iteration), Policy Iteration alternates between policy evaluation (computing the utility of a given policy) and policy improvement (updating the policy based on those utilities)\n",
    "\n",
    "![Value_Interation](photos/Policy_Iteration_Algorithm.jpg)\n",
    "\n",
    "i. Explain in your own words what happens in each step of the Policy Iteration algorithm above. Distinguish clearly between policy evaluation and policy improvement.\n",
    "\n",
    "ii. Define the following terms and explain their role in Policy Iteration:\n",
    "\n",
    "- Discount factor ($\\gamma$)\n",
    "- Policy evaluation\n",
    "- Policy improvement\n",
    "- Convergence criterion\n",
    "\n",
    "iii. Prove (or provide a reasoning argument) that the Policy Iteration algorithm converges to the optimal policy $\\pi^{*}$ in a finite number of steps.\n",
    "\n",
    "iv. Compare the influence of the discount factor $\\gamma$ on Policy Iteration versus Value Iteration. Which algorithm tends to converge faster, and why?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972582e3",
   "metadata": {},
   "source": [
    "## Q2. Policy Iteration for Markov Decision Processes\n",
    "\n",
    "### i) What happens in each step (and what is “evaluation” vs “improvement”)\n",
    "\n",
    "Policy Iteration starts by picking a policy \\(\\pi\\) (often random) and an initial utility vector \\(U\\) (often zeros). Then it loops through two phases again and again: first it figures out how good the current policy is, and then it tries to make the policy better using what it learned.\n",
    "\n",
    "In the **policy evaluation** step, the algorithm runs `POLICY-EVALUATION(π, U, mdp)` and replaces \\(U\\) with utilities that match the current policy \\(\\pi\\). Intuitively, this is like saying: *“If I commit to following this policy forever, how much total reward should I expect starting from each state?”* This step does not change the policy—it only measures it.\n",
    "\n",
    "After that comes the **policy improvement** step. The algorithm assumes the just-computed utilities \\(U\\) are correct for the current policy, and then for each state \\(s\\), it looks at all possible actions and finds the action \\(a^*\\) that gives the highest one-step lookahead value (the best Q-value under \\(U\\)). If this best action \\(a^*\\) is better than the action the current policy is already taking in that state, it updates the policy at that state: \\(\\pi(s) \\leftarrow a^*\\). If no state changes, the policy is “stable” and the loop stops.\n",
    "\n",
    "So the rhythm is:\n",
    "- **Evaluation:** “How good is my current policy?”\n",
    "- **Improvement:** “Given that, can I pick a better action anywhere?”\n",
    "\n",
    "---\n",
    "\n",
    "### ii) Key terms and their roles\n",
    "\n",
    "The **discount factor \\(\\gamma\\)** (with \\(0 \\le \\gamma < 1\\)) controls how much future rewards matter. It makes the long-run total return well-defined and is also what makes policy evaluation behave nicely (future contributions shrink geometrically). Larger \\(\\gamma\\) means the agent cares more about long-term consequences.\n",
    "\n",
    "**Policy evaluation** is the step that computes \\(U^\\pi\\): the utility of each state assuming you follow policy \\(\\pi\\). In many implementations this is done by solving a system of linear equations, or by iterative updates until the utilities are accurate enough. The key point is that you are not “optimizing” yet—you are just measuring the current policy.\n",
    "\n",
    "**Policy improvement** is the step where you update \\(\\pi\\) by making it greedy with respect to the evaluated utilities. In each state, you choose the action that looks best assuming the future utilities are \\(U\\). This is where the policy gets closer to optimal.\n",
    "\n",
    "The **convergence criterion** in the pseudocode is the `unchanged?` flag: if, after scanning all states, the policy never changes, then the policy is stable (already greedy w.r.t. its own utilities). At that point policy iteration stops and returns \\(\\pi\\).\n",
    "\n",
    "---\n",
    "\n",
    "### iii) Why Policy Iteration converges to the optimal policy in finitely many steps\n",
    "\n",
    "A clean way to reason about finite convergence is:\n",
    "\n",
    "1) **There are only finitely many deterministic policies.**  \n",
    "If the state space is finite and each state has finitely many actions, then the number of possible policies \\(\\pi\\) is finite (roughly \\(\\prod_{s\\in S} |A(s)|\\)).\n",
    "\n",
    "2) **Policy improvement never makes the policy worse (it improves or keeps it).**  \n",
    "After policy evaluation gives you \\(U^\\pi\\), the improvement step chooses actions that maximize the expected “immediate reward + discounted future utility.” This guarantees the new policy \\(\\pi'\\) is at least as good as \\(\\pi\\) everywhere, and typically strictly better in at least one state if any change is made. Intuitively: if you switch to an action that has higher expected return given accurate utilities, you should not end up with a worse long-run policy.\n",
    "\n",
    "3) **No cycling (under standard assumptions).**  \n",
    "Because each improvement step produces a policy that is strictly better whenever it changes anything, you can’t keep revisiting the same policy in a loop (that would imply no net improvement). With a finite set of policies and strictly improving steps, the algorithm must terminate after a finite number of improvements.\n",
    "\n",
    "4) **When it stops, it is optimal.**  \n",
    "If the policy doesn’t change during improvement, that means: for every state, the policy’s action is already the best action according to the correct utilities for that policy. That “stable greedy” condition corresponds to satisfying the Bellman optimality condition, so the policy must be optimal. In other words, the stop condition means: *there’s no one-step improvement anywhere*, which (for discounted finite MDPs) implies optimality.\n",
    "\n",
    "So: finite policies + monotonic improvements + no cycles ⇒ termination in finitely many iterations at \\(\\pi^*\\).\n",
    "\n",
    "---\n",
    "\n",
    "### iv) Discount factor effects: Policy Iteration vs Value Iteration (and which is faster)\n",
    "\n",
    "Both algorithms use \\(\\gamma\\) in the same conceptual way: it controls how much future matters and helps guarantee nice convergence properties. But \\(\\gamma\\) affects their *practical speed* differently.\n",
    "\n",
    "For **Value Iteration**, larger \\(\\gamma\\) usually slows convergence a lot because the update is a “small step” toward optimal utilities each sweep. When \\(\\gamma\\) is close to 1, the algorithm propagates information slowly across time because far-future consequences matter heavily, so utilities take many iterations to settle.\n",
    "\n",
    "For **Policy Iteration**, the effect of \\(\\gamma\\) shows up mostly inside **policy evaluation** (because evaluation must account for a longer effective horizon when \\(\\gamma\\) is large). However, Policy Iteration often needs **far fewer outer loops** than Value Iteration because each improvement step can make a “big jump” by changing many actions at once after a full evaluation.\n",
    "\n",
    "So in practice:\n",
    "- **Policy Iteration tends to converge in fewer iterations (outer loops)**, often much faster in iteration count.\n",
    "- **But each iteration is heavier**, because policy evaluation can be expensive (solving equations or many evaluation sweeps).\n",
    "- **Value Iteration tends to do cheap iterations but needs many more of them**, especially when \\(\\gamma\\) is close to 1.\n",
    "\n",
    "Which is faster overall depends on problem size and implementation, but a common rule of thumb is:\n",
    "- **Policy Iteration often converges faster in number of iterations**, because it makes policy-level jumps.\n",
    "- **Value Iteration can be simpler and cheaper per step**, but may require many steps, especially for large \\(\\gamma\\).\n",
    "\n",
    "If you’re thinking intuitively: Value Iteration is like “slowly shaping the landscape,” while Policy Iteration is like “commit to a strategy, measure it well, then switch strategies decisively.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6818284",
   "metadata": {},
   "source": [
    "__Q3.__\n",
    "\n",
    "![Value_Interation](photos/POMDP_Value_Iteration_Algorithm.jpg)\n",
    "\n",
    "Given the POMDP-VALUE-ITERATION function shown in Figure 16.16, consider a simple autonomous robot navigation scenario where the robot has uncertain sensor readings about obstacles and must navigate to a goal while avoiding collisions.\n",
    "\n",
    "i. Explain the significance of starting with one-step plans [a]. Why does the algorithm initialize $U'$ with all possible single actions rather than starting with empty plans or random plans? How does this relate to the principle of dynamic programming in POMDPs?\n",
    "\n",
    "ii. Analyze the utility vector computation $\\alpha_{[a]}(s) = \\sum_{s'} P(s'|s,a) R(s,a,s')$. This represents the expected utility of taking action $a$ in each state $s$. In our robot navigation context, if the robot has actions {move-forward, turn-left, turn-right} and states representing different obstacle configurations, what does this computation tell us about each action's value across different true world states?\n",
    "\n",
    "iii. The algorithm constructs plans consisting of \"an action and, for each possible next percept, $a$ plan in $U$.\"\n",
    "\n",
    "- Explain why plans must be conditioned on percepts rather than states in POMDPs\n",
    "- If our robot can observe {obstacle-detected, clear-path, goal-visible}, construct a sample 2-step plan and explain how it would be executed given different sensor readings\n",
    "Why does this tree-like plan structure grow exponentially with the planning horizon?\n",
    "\n",
    "iv. Critical Analysis of the REMOVE-DOMINATED-PLANS step:\n",
    "\n",
    "- Explain what it means for one plan to dominate another in the context of utility vectors over belief states\n",
    "- Why is this pruning step essential for computational tractability? What would happen if we skipped this step?\n",
    "- The convergence criterion MAX-DIFFERENCE $(U,U') \\leq \\epsilon(1 - \\gamma)/\\gamma$ is based on the maximum difference in utility vectors. Explain why this specific threshold guarantees that the policy derived from $U$ is $\\epsilon$-optimal, and how the discount factor $\\gamma$ influences when the algorithm terminates.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84f3b3",
   "metadata": {},
   "source": [
    "## Q3. POMDP Value Iteration (robot navigation with noisy sensors)\n",
    "\n",
    "### i) Why start with one-step plans \\([a]\\)?\n",
    "\n",
    "In a POMDP you don’t plan as “state → action” because you don’t know the true state. You plan as “belief → action,” where a belief is a probability distribution over states. The value function over beliefs is built from *plans* (policy trees), and the algorithm represents those plans using utility (alpha) vectors.\n",
    "\n",
    "Starting with all one-step plans \\([a]\\) is the simplest meaningful base case: “take action \\(a\\) now, and then stop.” From there, longer plans can be constructed by *backing up* (extending) existing plans by one more step. If you started with empty or random plans, you wouldn’t have a guaranteed complete set of immediate choices to extend, and you could miss the optimal first action. This is the dynamic programming idea: you build horizon-\\(t+1\\) solutions from horizon-\\(t\\) solutions. One-step plans are the horizon-1 foundation.\n",
    "\n",
    "So the initialization with all single actions is basically saying: “At minimum, the agent must be able to choose any immediate action; then we’ll grow those choices into longer contingent strategies.”\n",
    "\n",
    "---\n",
    "\n",
    "### ii) Meaning of \\(\\alpha_{[a]}(s) = \\sum_{s'} P(s'|s,a)\\,R(s,a,s')\\) in robot navigation\n",
    "\n",
    "This alpha-vector is telling you: *if the true world state were \\(s\\), and I take action \\(a\\) right now, what is my expected immediate payoff (averaging over what next state \\(s'\\) I might land in)?*\n",
    "\n",
    "In your robot setting, states \\(s\\) encode the “real” obstacle configuration (even if the robot can’t directly see it). For each action—move-forward, turn-left, turn-right—this computation produces a number for every possible true configuration:\n",
    "\n",
    "- If \\(s\\) is “obstacle directly ahead,” then **move-forward** likely leads to collision and a big negative reward, so \\(\\alpha_{[move-forward]}(s)\\) would be low (very negative).\n",
    "- If \\(s\\) is “clear path ahead,” then **move-forward** might get positive progress reward (or avoid time penalty), so its value is higher.\n",
    "- If \\(s\\) is “goal to the left,” then **turn-left** might set you up well (higher expected reward over the next state), while turning right might be less useful.\n",
    "\n",
    "So conceptually: this alpha-vector is an action’s “scorecard,” listing how good that action is under each possible hidden reality of the world.\n",
    "\n",
    "---\n",
    "\n",
    "### iii) Why plans are conditioned on *percepts* (observations), and a sample 2-step plan\n",
    "\n",
    "**Why percepts, not states?**  \n",
    "Because the agent cannot condition its next choice on the true state (it doesn’t know it). The only thing it gets after acting is an observation like “obstacle-detected” or “clear-path.” So a valid plan must say: “After I act, depending on what I *observe*, I will do X.”\n",
    "\n",
    "That’s why the plan structure is tree-like: an action first, then branches for each possible percept, each branch containing the next sub-plan.\n",
    "\n",
    "**Example 2-step plan** with observations {obstacle-detected, clear-path, goal-visible}:\n",
    "\n",
    "A sample plan could be:\n",
    "\n",
    "- Step 1: **move-forward**\n",
    "  - If observation = **goal-visible** → Step 2: **move-forward**\n",
    "  - If observation = **obstacle-detected** → Step 2: **turn-right**\n",
    "  - If observation = **clear-path** → Step 2: **move-forward**\n",
    "\n",
    "How it executes:\n",
    "- The robot takes move-forward once.\n",
    "- It receives a noisy sensor reading (one of the three).\n",
    "- It follows the branch corresponding to that reading and executes the second action.\n",
    "\n",
    "**Why does it grow exponentially with horizon?**  \n",
    "At each time step you choose one action, but then you must prepare for every possible observation outcome. If there are \\(|O|\\) possible percepts, then each additional step multiplies the number of branches roughly by \\(|O|\\). So horizon \\(h\\) plans are like trees with branching factor \\(|O|\\), giving exponential growth in the number of contingent futures the plan must specify.\n",
    "\n",
    "---\n",
    "\n",
    "### iv) REMOVE-DOMINATED-PLANS: what “domination” means and why pruning matters, plus the stopping threshold\n",
    "\n",
    "**What does it mean for one plan to dominate another?**  \n",
    "Each plan corresponds to an alpha-vector \\(\\alpha\\). Given a belief \\(b\\) (distribution over states), the value of that plan is basically the dot product \\(b \\cdot \\alpha\\). A plan (vector) \\(\\alpha_1\\) *dominates* \\(\\alpha_2\\) if it is never worse for any belief and is better for at least one belief. Intuitively: no matter what the robot believes about the world, plan 1 gives equal-or-higher expected return than plan 2, so plan 2 is useless.\n",
    "\n",
    "You can think of it as: \\(\\alpha_2\\) is a “dead strategy” because there is no situation (no belief) where it would ever be the best choice.\n",
    "\n",
    "**Why is pruning essential? What if we skip it?**  \n",
    "Without pruning, the number of plans explodes as you extend them (because of the observation-branching tree growth). Most of those plans are redundant—worse everywhere than some other plan. REMOVE-DOMINATED-PLANS deletes those redundant plans so you keep only the “useful frontier” of strategies. If you skip this step, memory and runtime typically blow up quickly, and the algorithm becomes computationally infeasible even for small horizons.\n",
    "\n",
    "**Why does the stopping test guarantee \\(\\epsilon\\)-optimality, and how does \\(\\gamma\\) affect termination?**  \n",
    "The MAX-DIFFERENCE condition is a practical way to ensure the value function over beliefs has stabilized enough that choosing the greedy action from the current set of alpha-vectors yields a policy within \\(\\epsilon\\) of optimal. The specific threshold with \\(\\epsilon(1-\\gamma)/\\gamma\\) comes from the same “discounted future error shrinks” idea as in standard value iteration: if your current update is only changing values by a small amount, then the remaining possible improvement over the infinite future is bounded because every further step is discounted by \\(\\gamma\\).\n",
    "\n",
    "As \\(\\gamma\\) gets closer to 1, future rewards matter more, so small residual errors can still influence long-run value significantly. That means the algorithm needs a *tighter* stopping condition (it must iterate longer). When \\(\\gamma\\) is smaller, the future fades quickly, so the algorithm can stop earlier because remaining improvements are heavily discounted away.\n",
    "\n",
    "So: higher \\(\\gamma\\) usually means slower convergence / later termination; lower \\(\\gamma\\) usually means earlier termination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57370bb3",
   "metadata": {},
   "source": [
    "__Q4__\n",
    "\n",
    "![Value_Interation](photos/Passive_ADP_Learner.jpg)\n",
    "\n",
    "i. Walk through the algorithm step-by-step. Why does it only update when encountering a \"new\" state ($s'$ is new), and what happens when it revisits a previously seen state? What's the significance of this design choice?\n",
    "\n",
    "ii. Explain the role of the utility table $U$ and outcome count table N in the learning process. How does incrementing $N_{s'|s,a}[s,a][s']$ and the normalization step help the agent improve its policy over time?\n",
    "\n",
    "\n",
    "iii. The algorithm calls POLICY-EVALUATION $(\\pi, U, mdp)$ but doesn't explicitly show policy improvement. How do you think this passive learner eventually converges to an optimal policy? What are the limitations of this \"passive\" approach compared to active learning?\n",
    "\n",
    "\n",
    "iv. If you were to implement this algorithm in a real-world scenario (like a robot learning to navigate or a game AI), what challenges might you face? Consider issues like convergence speed, memory requirements, and the assumption of a \"fixed policy\" during learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e13e4",
   "metadata": {},
   "source": [
    "## Passive ADP Learner (Adaptive Dynamic Programming) — intuitive explanation\n",
    "\n",
    "### i) Step-by-step walk-through, and why it treats “new” states specially\n",
    "\n",
    "This algorithm is a **passive** reinforcement learner: it does *not* choose actions to explore or improve; it just follows a **fixed policy** \\(\\pi\\) and tries to learn the MDP model (transitions + rewards) and the utilities that policy achieves.\n",
    "\n",
    "Each time step it receives a percept that includes the **current state** \\(s'\\) and **reward** \\(r\\).\n",
    "\n",
    "- If \\(s'\\) is a **new state** (first time ever seen), it initializes a utility entry for it, typically \\(U[s'] \\leftarrow 0\\). That’s not saying the state is worth zero forever—just “I don’t know yet, start somewhere.” The main reason to do this only for new states is practical: you only need to allocate/init data structures once per state.\n",
    "\n",
    "- If there *was* a previous state-action pair \\((s,a)\\) (i.e., this isn’t the very first percept), then you can treat the transition \\((s,a)\\to s'\\) as a data point and update your learned model:\n",
    "  - Increment the count of seeing next state \\(s'\\) after taking action \\(a\\) in \\(s\\).\n",
    "  - Store (or update) the reward associated with that transition.\n",
    "\n",
    "When the algorithm **revisits a previously seen state**, it does not “skip learning.” It still:\n",
    "- updates the transition counts again,\n",
    "- refines the estimated transition probabilities via normalization,\n",
    "- and re-runs policy evaluation to refine utilities.\n",
    "\n",
    "So “new state” only matters for **initialization** (creating an entry in \\(U\\), expanding the known state set, etc.). The significance is that the agent is building the MDP model online: as it discovers more states, its model grows, and its utility estimates become defined for a larger part of the world.\n",
    "\n",
    "---\n",
    "\n",
    "### ii) Role of the utility table \\(U\\) and outcome count table \\(N\\)\n",
    "\n",
    "The utility table \\(U\\) is the agent’s running estimate of “how good it is” to be in each state *when following the fixed policy* \\(\\pi\\). It’s not directly learned from raw rewards by averaging; instead, it’s computed by solving the Bellman equations for the estimated model (that’s what policy evaluation does). So \\(U\\) is like: “given what I currently believe about transitions and rewards, what long-term return does policy \\(\\pi\\) produce from each state?”\n",
    "\n",
    "The outcome count table \\(N\\) is how the agent learns the **transition model** from experience. The entry \\(N_{s'|s,a}[s,a][s']\\) is literally a counter: “how many times did I take action \\(a\\) in state \\(s\\) and end up in \\(s'\\)?” Each increment makes the model estimate more accurate.\n",
    "\n",
    "The normalization step turns counts into probabilities:\n",
    "- If from \\((s,a)\\) you’ve seen outcomes \\(\\{s'_1, s'_2, ...\\}\\) with counts, normalizing gives an empirical estimate of \\(P(s' \\mid s,a)\\).\n",
    "- As the robot experiences more transitions, those empirical probabilities usually get closer to the true environment dynamics (assuming the environment is stationary and you see enough samples).\n",
    "\n",
    "So the loop is: **experience → better \\(P\\) and \\(R\\) estimates → better policy evaluation → better \\(U\\) estimates (for that fixed policy).**\n",
    "\n",
    "One subtle point: this helps the agent “improve” its understanding, but not necessarily its behavior, because behavior is locked to \\(\\pi\\).\n",
    "\n",
    "---\n",
    "\n",
    "### iii) It doesn’t show policy improvement — so does it ever become optimal? What are the passive limitations?\n",
    "\n",
    "With the algorithm exactly as shown (fixed \\(\\pi\\)), it does **not** naturally converge to the optimal policy \\(\\pi^*\\) unless \\(\\pi\\) was already optimal. What it converges to is:\n",
    "- an increasingly accurate model \\((\\hat P, \\hat R)\\) for the parts of the state space it visits under \\(\\pi\\),\n",
    "- and the correct utilities \\(U^\\pi\\) for that same policy (again, for the visited region).\n",
    "\n",
    "So how could it *eventually* lead to an optimal policy in practice? Usually one of these is true in a broader system:\n",
    "- You run this passive learner to learn a model, *then* you run planning (value iteration / policy iteration) on the learned model to compute a better policy.\n",
    "- Or you alternate: learn model → compute improved policy → follow new policy → learn more (that becomes *active* ADP / model-based RL).\n",
    "\n",
    "Limitations of being passive:\n",
    "- **Exploration problem:** If \\(\\pi\\) never tries an action or never visits a region, the model for those parts stays wrong/unknown forever.\n",
    "- **No guarantee of optimality:** You can perfectly learn \\(U^\\pi\\) and still be far from \\(\\pi^*\\).\n",
    "- **Biased data:** The learned model is “on-policy”—it reflects what your fixed behavior happens to experience, not the full MDP.\n",
    "\n",
    "Active learners (e.g., explore, epsilon-greedy, optimism/bonuses, Thompson sampling) deliberately gather information that improves decision quality, not just model accuracy under a fixed behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### iv) Real-world implementation challenges (robot navigation / game AI)\n",
    "\n",
    "In real systems, a few issues show up quickly:\n",
    "\n",
    "- **Convergence speed:** Policy evaluation can be expensive if you re-run it after every single new transition. In practice you’d often do it periodically, or do incremental / approximate updates rather than full evaluation each step.\n",
    "\n",
    "- **Memory footprint:** The count table \\(N[s,a,s']\\) can be huge: it scales like \\(|S|\\times|A|\\times|S|\\) in the worst case. That’s often impossible for robotics or large games. People use sparse structures, function approximation, learned dynamics models, or compress state representations.\n",
    "\n",
    "- **State representation is rarely clean:** In robotics, the “state” is continuous or high-dimensional (lidar scans, images, pose). You can’t just use a table indexed by \\(s\\). You need discretization, feature-based models, or neural approximations. Once you do that, the clean theoretical guarantees weaken.\n",
    "\n",
    "- **Non-stationarity:** Real environments change (sensor drift, moving obstacles, game opponents adapting). Counts from the distant past might become misleading. You may need forgetting factors or sliding windows.\n",
    "\n",
    "- **Fixed policy assumption is restrictive:** If the fixed policy is mediocre, you may never see important states (like the goal region or risky corners), so the learned model stays incomplete. That can make the whole learning process look “stuck.”\n",
    "\n",
    "- **Reward assignment noise:** The pseudocode sets \\(R[s,a,s'] \\leftarrow r\\). In the real world rewards can be noisy; you’d often average rewards per transition (or per state-action) rather than overwrite.\n",
    "\n",
    "Overall, the passive ADP learner is conceptually clean and great for learning “how good is this policy?” but in most practical RL systems you eventually need some form of **policy improvement + exploration** to get strong performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218a0ca",
   "metadata": {},
   "source": [
    "__Q5__\n",
    "\n",
    "![Value_Interation](photos/Passive_TD_Learner.jpg)\n",
    "\n",
    "i. Examine the utility update formula: $U[s] \\leftarrow U[s] + \\alpha (N_s[s]) \\times (r + \\gamma U[s'] - U[s])$. Break down each component. What does $(r + \\gamma U[s'] - U[s])$ represent conceptually, and why is this called a \"temporal difference\"?\n",
    "\n",
    "ii. The algorithm uses $\\alpha(N_s[s])$ as a step-size function that depends on how often state $s$ has been visited. Why is this important for convergence? What would happen if we used a constant learning rate instead, and how might that affect the agent's learning?\n",
    "\n",
    "iii. How does this TD approach differ from the Passive-ADP-Learner? Which algorithm would you expect to learn faster, and which would be more memory-efficient? Consider the trade-offs between model-based and model-free learning.\n",
    "\n",
    "iv. Notice that the algorithm uses $U[s']$ (the current utility estimate of the next state) to update $U[s]$. This is called \"bootstrapping\" which means learning from your own estimates. What are the advantages and potential risks of this approach? How does it enable learning without knowing the full transition model?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db63d9",
   "metadata": {},
   "source": [
    "## Q5. Passive Temporal-Difference (TD) Learning\n",
    "\n",
    "### i) Breaking down the TD utility update\n",
    "\n",
    "The core update rule is:\n",
    "\n",
    "U[s] ← U[s] + α(Nₛ[s]) · (r + γU[s′] − U[s])\n",
    "\n",
    "This update is trying to *nudge* the current utility estimate of state \\(s\\) in a better direction based on what was just observed.\n",
    "\n",
    "The term \\(r + \\gamma U[s']\\) is the agent’s **one-step lookahead estimate** of how good state \\(s\\) turned out to be: it combines the immediate reward \\(r\\) received after leaving \\(s\\), plus the discounted estimate of how good the next state \\(s'\\) is believed to be. This is essentially a guess of what the true utility of \\(s\\) *should* be, based on the most recent experience.\n",
    "\n",
    "The subtraction \\(r + \\gamma U[s'] - U[s]\\) is the **prediction error**: it measures how surprised the agent is. If this quantity is positive, the outcome was better than expected; if negative, worse than expected. This difference is called a *temporal difference* because it compares predictions made at two different time steps: the estimate of \\(U[s]\\) made in the past versus a new estimate based on information one step into the future.\n",
    "\n",
    "The learning rate α then controls how strongly this error should influence the current estimate.\n",
    "\n",
    "---\n",
    "\n",
    "### ii) Why the step-size depends on visit counts\n",
    "\n",
    "The learning rate α(Nₛ[s]) depends on how many times state \\(s\\) has been visited. Early on, when \\(s\\) has been seen only a few times, the agent should learn aggressively because its estimate is very uncertain. As the number of visits grows, the learning rate shrinks, making updates more conservative and preventing large oscillations once the estimate has mostly stabilized.\n",
    "\n",
    "This decreasing step-size is crucial for convergence: it ensures that the agent eventually “settles down” instead of endlessly chasing noise in rewards or transitions.\n",
    "\n",
    "If a **constant learning rate** were used instead, the agent would continue to make sizable updates forever. This can make learning faster initially, but it often prevents true convergence: the utility estimates keep fluctuating around the correct value. In practice, constant learning rates are sometimes used when the environment is non-stationary, but for guaranteed convergence in a stationary MDP, diminishing step sizes are safer.\n",
    "\n",
    "---\n",
    "\n",
    "### iii) TD learning vs Passive ADP learning\n",
    "\n",
    "The key difference is that **Passive ADP is model-based**, while **TD learning is model-free**.\n",
    "\n",
    "Passive ADP explicitly learns:\n",
    "- transition probabilities (via count tables),\n",
    "- rewards,\n",
    "- and then recomputes utilities by running policy evaluation on the learned model.\n",
    "\n",
    "TD learning skips all of that. It never builds an explicit model of the environment. Instead, it directly updates utility estimates from raw experience using the TD error.\n",
    "\n",
    "Because of this:\n",
    "- **TD learning is more memory-efficient**: it only stores a utility table (and visit counts), not full transition matrices.\n",
    "- **TD learning usually learns faster initially**, because each experience immediately updates utilities without waiting to estimate a full model or solve Bellman equations.\n",
    "\n",
    "On the other hand:\n",
    "- Passive ADP can eventually produce very accurate utility estimates if the model is learned well.\n",
    "- TD learning trades that accuracy for simplicity and speed, and its convergence relies more heavily on careful choice of learning rates.\n",
    "\n",
    "So the trade-off is classic:\n",
    "- Passive ADP: slower, heavier, but principled and model-based.\n",
    "- TD learning: faster, lighter, but more approximate and sensitive to parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### iv) Bootstrapping: benefits and risks\n",
    "\n",
    "Using \\(U[s']\\) to update \\(U[s]\\) means the algorithm is **bootstrapping**—it learns from its own current estimates rather than waiting for full returns. This has a major advantage: the agent does not need to wait until the end of an episode to learn something useful. Learning happens incrementally, step by step, which makes TD methods highly practical in ongoing or infinite-horizon tasks.\n",
    "\n",
    "Bootstrapping is also what allows TD learning to work **without knowing the transition model**. The agent never needs \\(P(s'|s,a)\\); it only needs the observed next state and reward. The expectation over next states is implicitly approximated by sampling through experience.\n",
    "\n",
    "The risk is that early utility estimates may be poor, and bootstrapping from them can propagate errors. If learning rates are too high or exploration is insufficient, the agent may converge slowly or to biased estimates. In unstable settings, bootstrapping can also amplify noise.\n",
    "\n",
    "Despite these risks, bootstrapping is one of the main reasons TD learning scales well to large problems and forms the foundation of many modern reinforcement learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a8333",
   "metadata": {},
   "source": [
    "__Q6__\n",
    "\n",
    "![Value_Interation](photos/Q-Learning_Agent.jpg)\n",
    "\n",
    "i. Compare this algorithm to the passive learners (passive ADP learners and passive TD learners). How does the action selection using $argmax_{a'} f(Q[s',a'], N_{sa}[s',a'])$ make this an \"active\" learner? What is the agent now optimizing for that it wasn't before?\n",
    "\n",
    "ii. Examine the Q-value update: $Q[s,a] \\leftarrow Q[s,a] + \\alpha (N_{sa}[s,a])(r + \\gamma max_{a'} Q[s',a'] - Q[s,a])$. Why does this use $max_{a'} Q[s',a']$ instead of following a fixed policy like the previous algorithms? What does this mathematical difference represent conceptually?\n",
    "\n",
    "iii. The algorithm uses an exploration function $f(Q[s',a'], N_{sa}[s',a'])$ to choose actions. Why is exploration crucial in Q-learning? What might happen if the agent always chose the action with the highest current Q-value? Design a simple exploration function and justify your choice.\n",
    "\n",
    "iv. Unlike ADP-learner, this algorithm doesn't need to learn transition probabilities or call POLICY-EVALUATION. What are the practical advantages of this model-free approach? In what scenarios would you prefer Q-learning over model-based methods, and why might this be particularly valuable in real-world applications?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce373a30",
   "metadata": {},
   "source": [
    "## Q6. Exploratory Q-Learning Agent\n",
    "\n",
    "### i) Why this is an *active* learner (vs passive ADP / passive TD)\n",
    "\n",
    "The key difference between this algorithm and the passive learners is **who controls the actions**. In passive ADP and passive TD learning, the agent follows a *fixed policy* \\(\\pi\\) and only learns from whatever experience that policy happens to generate. The agent’s goal there is limited: estimate utilities (or a model) **for that given policy**, not necessarily to behave optimally.\n",
    "\n",
    "In contrast, the Q-learning agent *chooses its own actions* using  \n",
    "\\(\\arg\\max_{a'} f(Q[s',a'], N_{sa}[s',a'])\\).  \n",
    "This means the agent is actively balancing two objectives:\n",
    "- exploiting actions that currently look good (high Q-values),\n",
    "- exploring actions that are uncertain or under-tried (low visit counts).\n",
    "\n",
    "Because the agent is now deciding *what to try next*, it is no longer just evaluating a policy—it is directly searching for the **optimal behavior**. What it is optimizing for now is the *optimal action-value function* \\(Q^*(s,a)\\), not the value of some fixed policy.\n",
    "\n",
    "---\n",
    "\n",
    "### ii) Why the update uses `max Q` instead of a fixed policy\n",
    "\n",
    "The Q-learning update is:\n",
    "\n",
    "Q[s,a] ← Q[s,a] + α(Nₛₐ[s,a]) · (r + γ maxₐ′ Q[s′,a′] − Q[s,a])\n",
    "\n",
    "The crucial difference from passive TD learning is the term  \n",
    "\\(\\max_{a'} Q[s',a']\\).\n",
    "\n",
    "In passive methods, updates assume the agent will continue following a fixed policy, so future value is computed according to that policy. Here, Q-learning instead assumes that **from the next state onward, the agent will behave optimally**, regardless of what action it actually takes during learning.\n",
    "\n",
    "Conceptually, this means:\n",
    "- Passive learners learn “how good things are if I keep behaving this way.”\n",
    "- Q-learning learns “how good things *could be* if I always make the best possible choice from now on.”\n",
    "\n",
    "This mathematical change is what allows Q-learning to converge to the optimal policy even while the agent is behaving non-optimally during exploration.\n",
    "\n",
    "---\n",
    "\n",
    "### iii) Why exploration is crucial, and a simple exploration function\n",
    "\n",
    "Exploration is essential because Q-learning relies on *experience* to discover which actions are truly good. If the agent always chose the action with the highest current Q-value, it could easily get stuck with a **wrong belief** early on. For example, if an action happens to give a decent reward the first few times, the agent may never try alternatives that are actually better in the long run.\n",
    "\n",
    "Without exploration:\n",
    "- some state–action pairs may never be visited,\n",
    "- their Q-values remain inaccurate,\n",
    "- and the agent may converge to a suboptimal policy.\n",
    "\n",
    "A simple exploration function could be:\n",
    "\n",
    "f(Q, N) =\n",
    "- return a very large value if N < k  \n",
    "- otherwise return Q\n",
    "\n",
    "This forces the agent to try each action at least \\(k\\) times in every state before trusting its Q-values. The justification is intuitive: you don’t trust an estimate until you’ve seen enough data. After sufficient exploration, the agent naturally shifts toward exploitation.\n",
    "\n",
    "Other common choices (like ε-greedy or UCB-style bonuses) follow the same idea: encourage uncertainty-seeking early, and confident exploitation later.\n",
    "\n",
    "---\n",
    "\n",
    "### iv) Advantages of being model-free (and when to prefer Q-learning)\n",
    "\n",
    "Unlike ADP-based learners, Q-learning does **not**:\n",
    "- store transition probabilities,\n",
    "- estimate a reward model,\n",
    "- or run policy evaluation.\n",
    "\n",
    "This has several practical advantages:\n",
    "- **Lower memory usage:** no need for large \\(P(s'|s,a)\\) tables.\n",
    "- **Simpler implementation:** only Q-values and visit counts.\n",
    "- **Online learning:** updates happen immediately after each step.\n",
    "- **Works with unknown dynamics:** the agent doesn’t need to know how the world works internally.\n",
    "\n",
    "Q-learning is especially attractive in real-world scenarios where:\n",
    "- the environment is complex or unknown (robotics, games, user behavior),\n",
    "- the state space is large or continuous (making model learning impractical),\n",
    "- dynamics may change over time (non-stationarity).\n",
    "\n",
    "Model-based methods can be more data-efficient when the model is small and accurate, but in many real systems the model is either too expensive to learn or simply unavailable. In those cases, Q-learning’s ability to learn optimal behavior *directly from experience* is a major advantage.\n",
    "\n",
    "In short: Q-learning trades explicit knowledge of the environment for flexibility, scalability, and practical power—which is why it sits at the core of many modern reinforcement learning systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f6aa13",
   "metadata": {},
   "source": [
    "**Q7. Mini Simplified Auction Game**\n",
    "\n",
    "We will create an AI agent using smolagent framework that competes in 2-round auction games against another agent. Our goal is to maximize the agent's total utility score by strategically bidding on items while managing uncertainty about opponents' strategies and values. In this game version, the requirements are as follows:\n",
    "\n",
    "- The number AI players: 2\n",
    "- The number of auction rounds per game: 2\n",
    "- Objective: The highest total utility score at the end of the game (after 2 rounds)\n",
    "- Starting conditions:\n",
    "\n",
    "    - Budget: each agent starts with 50 coins\n",
    "    - 2 items will be auctioned in sequence.\n",
    "        - Item A: Magic Book.\n",
    "        - Item B: Flying Carpet.\n",
    "\n",
    "    - Private valuations: Each agent has different private values for each item (assigned by the environment). The agent only knows its private valuations and it doesn't know opponents' valuations (this is the uncertainty the agent has to handle)\n",
    "\n",
    "- The game rules:\n",
    "\n",
    "    - Minimum starting bid = 3 coins\n",
    "    - Minimum raise = 5 coin (each new bid must be at least 5 coin higher)\n",
    "    - Players bid in rotating order (taking turns)\n",
    "    - Players can pass to exit the current item's auction permanently\n",
    "    - Once you pass, you cannot re-enter that specific item's auction\n",
    "    - You can still participate in future item auctions\n",
    "    - The auction continues until only one active bidder remains\n",
    "    - The last remaining bidder wins and pays their final bid amount\n",
    "\n",
    "- The total utility score formula = Sum of private values of all items won + Remaining coins $\\times$ 0.1\n",
    "\n",
    "\n",
    "*Example:*\n",
    "\n",
    "We have 2 agents named Mickey and Minnie.\n",
    "\n",
    "At the beginning, each private valuations (fixed at the game start) are given to the each agent. Please note that the agent only knows its private valuations and it doesn't know opponents' valuations\n",
    "\n",
    "- Mickey's private valuations:\n",
    "    - Item A (Magic Book): $\\textcolor{teal} {30}$ coins\n",
    "    - Item B (Flying Carpet): $25$ coins\n",
    "\n",
    "- Minnie's private valuations:\n",
    "    - Item A (Magic Book): $20$ coins\n",
    "    - Item B (Flying Carpet): $20$ coins\n",
    "\n",
    "Let's go through one auction, item A - Magic Book.\n",
    "\n",
    "Item A (Magic Book) auction:\n",
    "- Mickey bids $8$ coins.\n",
    "- Minnie bids $13$ coins.\n",
    "- Mickey bids $25$ coins.\n",
    "- Minnie passes.\n",
    "\n",
    "$\\rightarrow$ Result: Mickey got Magic Book for $25$ coins and he has only $50 - 25 = 25$ coins left.\n",
    "\n",
    "Now, item B - Flying Carpet.\n",
    "\n",
    "Item B (Flying Carpet) auction:\n",
    "- Minnie bids $10$ coins (values it at $20$).\n",
    "- Mickey bids $15$ coins.\n",
    "- Minnie bids $20$ coins (maximum she should pay).\n",
    "- Mickey bids $25$ coins.\n",
    "- Minnie gets frustrated and bids $35$ coins (OVERBIDDING - mistake!).\n",
    "- Mickey passes (smart move - won't pay more than it's worth to him).\n",
    "\n",
    "$\\rightarrow$ Minnie wins for $35$ coins - she paid $15$ more than it's worth to her!\n",
    "\n",
    "Here are the items and the costs each agent obtained:\n",
    "\n",
    "| Item | Owner | Cost |\n",
    "|:-----|:------|:-----|\n",
    "| A | Mickey | $25$ coins |\n",
    "| B | Minnie | $35$ coins |\n",
    "\n",
    "And the remaining amount of money at the end:\n",
    "\n",
    "| Agent | Remaining Amount |\n",
    "|:------|:-----------------|\n",
    "| Mickey | $50 - 25 = \\textcolor{orange} {25}$ |\n",
    "| Minnie | $50 - 35 = 15$ |\n",
    "\n",
    "The total utility score:\n",
    "\n",
    "| Agent | Utility |\n",
    "|:------|:--------|\n",
    "| Mickey | $\\textcolor{teal} {30} + \\textcolor{orange} {25} \\times 0.1 = 32.5$ |\n",
    "| Minnie | $20 + 15 \\times 0.1 = 21.5$ |\n",
    "\n",
    "$\\rightarrow$ Mickey won!\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In every turn, the agent has to make a decision making, deal with uncertainty, and proactively learn the pattern.\n",
    "\n",
    "1. Should I bid on items I value less highly? (this is decision making)\n",
    "2. How much do opponents value this item? (this is uncertainty)\n",
    "3. What bidding patterns lead to victory? (learning and adaptation)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Ineffective strategies will get punished:\n",
    "- Overbidding: pay more than item's worth (private valuation) to you = spend more money but at the end you total utility is only equal to the private valuation (Minnie with item B).\n",
    "- Underbidding: miss items you value highly = missed opportunities.\n",
    "- Poor budget management: spend everything early = cannot compete later and cannot compete on items the agent valued high.\n",
    "- Ignoring your private valuations.\n",
    "- Spending all your budget on one item.\n",
    "- Bidding randomly high amounts or playing without strategy.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Effective strategies are:\n",
    "- Strategic bidding based on \"your\" valuations.\n",
    "- Opponent modeling and uncertainty handling.\n",
    "- Smart budget allocation across all 2 rounds (maybe allocate more coins to the items you value highly)\n",
    "- Learning and adapting from game experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18507aec",
   "metadata": {},
   "source": [
    "You will implement the decision-making logic for an AI agent that competes in the mini auction game. Review the game rules, example, and strategy guidelines provided in the previous section before beginning. Your agent must make strategic bidding decisions to maximize utility while managing uncertainty about opponent valuations.\n",
    "\n",
    "Complete the decide_bid() method in the MiniStrategicAgent class. This method is called whenever your agent must decide whether to bid or pass. It must return a tuple: (action, amount) where action is BID or PASS, and amount is the bid value or None.\n",
    "\n",
    "Your agent has access to a get_game_state tool that returns structured data: your budget, current item name, your valuations, current bid, and minimum raise requirement. You must use this tool to gather information before making decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf130dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "\n",
    "# =========================\n",
    "# BidAction Enum\n",
    "# =========================\n",
    "class BidAction(Enum):\n",
    "    BID = \"BID\"\n",
    "    PASS = \"PASS\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Auction + Game State\n",
    "# =========================\n",
    "class Auction:\n",
    "    def __init__(self, item_name: str, agent_names):\n",
    "        self.item_name = item_name\n",
    "        self.current_bid = 0\n",
    "        self.active_bidders = set(agent_names)\n",
    "        self.passed_bidders = set()\n",
    "        self.last_bidder = None\n",
    "\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self, budget: int, valuations: Dict[str, int], auction: Auction):\n",
    "        self.budget = budget\n",
    "        self.valuations = valuations\n",
    "        self.current_auction = auction\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Mini Auction Environment\n",
    "# =========================\n",
    "class MiniAuctionEnvironment:\n",
    "    MIN_BID = 3\n",
    "    MIN_RAISE = 5\n",
    "\n",
    "    def __init__(self, agent_names, valuations_by_agent):\n",
    "        self.agent_names = agent_names\n",
    "\n",
    "        # Single auction (Item A)\n",
    "        self.game_state = None\n",
    "        auction = Auction(\"Magic Book\", agent_names)\n",
    "\n",
    "        # For simplicity: one shared GameState reference (as your code expects)\n",
    "        self.game_state = GameState(\n",
    "            budget=50,\n",
    "            valuations=valuations_by_agent[agent_names[0]],\n",
    "            auction=auction\n",
    "        )\n",
    "\n",
    "        # Per-agent state (used by tool)\n",
    "        self._states = {}\n",
    "        for name in agent_names:\n",
    "            self._states[name] = GameState(\n",
    "                budget=50,\n",
    "                valuations=valuations_by_agent[name],\n",
    "                auction=auction\n",
    "            )\n",
    "\n",
    "    def get_game_state(self, agent_name: str) -> Dict[str, Any]:\n",
    "        gs = self._states[agent_name]\n",
    "        a = gs.current_auction\n",
    "        return {\n",
    "            \"budget\": gs.budget,\n",
    "            \"current_item\": a.item_name,\n",
    "            \"current_bid\": a.current_bid,\n",
    "            \"min_raise\": self.MIN_RAISE,\n",
    "            \"valuations\": gs.valuations,\n",
    "            \"active_bidders\": list(a.active_bidders),\n",
    "            \"passed_bidders\": list(a.passed_bidders),\n",
    "        }\n",
    "\n",
    "    def apply_action(self, agent_name: str, action: BidAction, amount: Optional[int]):\n",
    "        gs = self._states[agent_name]\n",
    "        a = gs.current_auction\n",
    "\n",
    "        if agent_name not in a.active_bidders:\n",
    "            return\n",
    "\n",
    "        if action == BidAction.PASS:\n",
    "            a.active_bidders.remove(agent_name)\n",
    "            a.passed_bidders.add(agent_name)\n",
    "            print(f\"{agent_name} PASSES\")\n",
    "            return\n",
    "\n",
    "        if action == BidAction.BID:\n",
    "            delta = amount - a.current_bid\n",
    "            gs.budget -= delta\n",
    "            a.current_bid = amount\n",
    "            a.last_bidder = agent_name\n",
    "            print(f\"{agent_name} BIDS {amount}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Tool: GetGameStateTool\n",
    "# =========================\n",
    "class GetGameStateTool:\n",
    "    def __init__(self, env: MiniAuctionEnvironment):\n",
    "        self.env = env\n",
    "\n",
    "    def __call__(self, agent_name: str) -> Dict[str, Any]:\n",
    "        return self.env.get_game_state(agent_name)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Minimal CodeAgent (LLM Stub)\n",
    "# =========================\n",
    "class CodeAgent:\n",
    "    \"\"\"\n",
    "    Minimal stand-in for smolagents.CodeAgent.\n",
    "    It DOES NOT do reasoning — it simply:\n",
    "    - Calls get_game_state\n",
    "    - Returns a simple JSON-like dict\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tools, model=None, max_steps=5, additional_authorized_imports=None):\n",
    "        self.tool = tools[0]\n",
    "\n",
    "    def run(self, prompt: str):\n",
    "        # Extract agent name from prompt\n",
    "        name = prompt.split(\"You are \")[1].split(\",\")[0]\n",
    "        state = self.tool(name)\n",
    "\n",
    "        value = state[\"valuations\"][state[\"current_item\"]]\n",
    "        budget = state[\"budget\"]\n",
    "        current_bid = state[\"current_bid\"]\n",
    "        min_raise = state[\"min_raise\"]\n",
    "\n",
    "        # Very simple rational behavior\n",
    "        if current_bid == 0:\n",
    "            bid = max(10, MiniAuctionEnvironment.MIN_BID)\n",
    "            if bid <= value:\n",
    "                return {\"action\": \"BID\", \"amount\": bid}\n",
    "            return {\"action\": \"PASS\", \"amount\": None}\n",
    "\n",
    "        next_bid = current_bid + min_raise\n",
    "        if next_bid <= value and next_bid <= budget:\n",
    "            return {\"action\": \"BID\", \"amount\": next_bid}\n",
    "\n",
    "        return {\"action\": \"PASS\", \"amount\": None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811723c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import json\n",
    "import re\n",
    "\n",
    "class MiniStrategicAgent:\n",
    "    def __init__(self, name: str, env, model):\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "\n",
    "        tools = [GetGameStateTool(env)]\n",
    "        self.agent = CodeAgent(\n",
    "            tools=tools,\n",
    "            model=model,\n",
    "            max_steps=5,\n",
    "            additional_authorized_imports=[\"json\"]\n",
    "        )\n",
    "\n",
    "    def decide_bid(self) -> tuple[\"BidAction\", Optional[int]]:\n",
    "        # --------- 1) Check if still active in this auction ----------\n",
    "        gs = getattr(self.env, \"game_state\", None)\n",
    "        ca = getattr(gs, \"current_auction\", None) if gs else None\n",
    "\n",
    "        # Try a few common patterns for “active bidders / passed bidders”\n",
    "        active_bidders = None\n",
    "        passed_bidders = None\n",
    "        if ca is not None:\n",
    "            active_bidders = getattr(ca, \"active_bidders\", None) or getattr(ca, \"active\", None)\n",
    "            passed_bidders = getattr(ca, \"passed_bidders\", None) or getattr(ca, \"passed\", None)\n",
    "\n",
    "        if active_bidders is not None and self.name not in set(active_bidders):\n",
    "            return (BidAction.PASS, None)\n",
    "        if passed_bidders is not None and self.name in set(passed_bidders):\n",
    "            return (BidAction.PASS, None)\n",
    "\n",
    "        # --------- helper: parse LLM output into {\"action\":..., \"amount\":...} ----------\n",
    "        def _extract_dict(result) -> dict:\n",
    "            if isinstance(result, dict):\n",
    "                return result\n",
    "\n",
    "            # smolagents sometimes returns AgentText / objects with .content / .text\n",
    "            text = None\n",
    "            for attr in (\"content\", \"text\", \"output\", \"final\"):\n",
    "                if hasattr(result, attr):\n",
    "                    v = getattr(result, attr)\n",
    "                    if isinstance(v, str) and v.strip():\n",
    "                        text = v\n",
    "                        break\n",
    "\n",
    "            if text is None:\n",
    "                text = str(result)\n",
    "\n",
    "            # Try JSON first\n",
    "            try:\n",
    "                obj = json.loads(text)\n",
    "                if isinstance(obj, dict):\n",
    "                    return obj\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Try to find a JSON object inside the text\n",
    "            m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "            if m:\n",
    "                try:\n",
    "                    obj = json.loads(m.group(0))\n",
    "                    if isinstance(obj, dict):\n",
    "                        return obj\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Very lightweight fallback parsing\n",
    "            # e.g. \"BID 15\" or \"PASS\"\n",
    "            upper = text.upper()\n",
    "            if \"PASS\" in upper:\n",
    "                return {\"action\": \"PASS\", \"amount\": None}\n",
    "            m2 = re.search(r\"\\bBID\\b\\D*(\\d+)\", upper)\n",
    "            if m2:\n",
    "                return {\"action\": \"BID\", \"amount\": int(m2.group(1))}\n",
    "            return {}\n",
    "\n",
    "        # --------- helper: legality + clamp ----------\n",
    "        def _legalize(action: str, amount: Optional[int], state: dict) -> tuple[\"BidAction\", Optional[int]]:\n",
    "            budget = int(state.get(\"budget\", 0))\n",
    "\n",
    "            current_bid = state.get(\"current_bid\", 0)\n",
    "            if current_bid is None:\n",
    "                current_bid = 0\n",
    "            current_bid = int(current_bid)\n",
    "\n",
    "            min_raise = state.get(\"min_raise\", None)\n",
    "            if min_raise is None:\n",
    "                min_raise = getattr(self.env, \"MIN_RAISE\", 5)\n",
    "            min_raise = int(min_raise)\n",
    "\n",
    "            min_bid = int(getattr(self.env, \"MIN_BID\", 3))\n",
    "\n",
    "            if str(action).upper() == \"PASS\":\n",
    "                return (BidAction.PASS, None)\n",
    "\n",
    "            # BID path\n",
    "            if amount is None:\n",
    "                return (BidAction.PASS, None)\n",
    "\n",
    "            try:\n",
    "                amount = int(amount)\n",
    "            except Exception:\n",
    "                return (BidAction.PASS, None)\n",
    "\n",
    "            if budget < min_bid:\n",
    "                return (BidAction.PASS, None)\n",
    "\n",
    "            # Must meet minimum starting bid or minimum raise\n",
    "            if current_bid <= 0:\n",
    "                min_allowed = min_bid\n",
    "            else:\n",
    "                min_allowed = current_bid + min_raise\n",
    "\n",
    "            if amount < min_allowed:\n",
    "                amount = min_allowed\n",
    "\n",
    "            if amount > budget:\n",
    "                return (BidAction.PASS, None)\n",
    "\n",
    "            return (BidAction.BID, amount)\n",
    "\n",
    "        # --------- 2) LLM strategic prompt (forces tool usage + strict JSON output) ----------\n",
    "        prompt = f\"\"\"\n",
    "You are {self.name}, an agent in a 2-round sequential auction (2 items total).\n",
    "Rules:\n",
    "- Budget starts at 50 coins.\n",
    "- Min starting bid = {getattr(self.env,'MIN_BID',3)}.\n",
    "- Min raise = {getattr(self.env,'MIN_RAISE',5)}.\n",
    "- Turn-based bidding; you may PASS and then you cannot re-enter for this item.\n",
    "- Utility = sum(values of items you win) + 0.1 * remaining_coins.\n",
    "\n",
    "Task:\n",
    "1) FIRST call the tool get_game_state to read the current situation.\n",
    "2) Decide BID or PASS strategically, managing budget across BOTH items.\n",
    "3) Never bid above your budget. Avoid overpaying when it harms your total utility\n",
    "   (especially if you might need budget for the next item).\n",
    "\n",
    "Return ONLY a JSON object like:\n",
    "{{\"action\":\"BID\",\"amount\":15}}\n",
    "or\n",
    "{{\"action\":\"PASS\",\"amount\":null}}\n",
    "\"\"\"\n",
    "\n",
    "        # --------- 3) Run agent (tool-using) ----------\n",
    "        try:\n",
    "            llm_result = self.agent.run(prompt)\n",
    "            decision = _extract_dict(llm_result)\n",
    "        except Exception:\n",
    "            decision = {}\n",
    "\n",
    "        # --------- 4) Get authoritative game state for validation ----------\n",
    "        # If LLM already used it, great; still call tool via env state if present.\n",
    "        # If your tool returns state only through the tool, you can rely on gs snapshot.\n",
    "        # We'll construct a minimal state dict from env/game_state as fallback.\n",
    "        state = {}\n",
    "        try:\n",
    "            # Prefer pulling from the environment (most reliable)\n",
    "            # If your GetGameStateTool stores last output somewhere, you can use that too.\n",
    "            if gs is not None:\n",
    "                state[\"budget\"] = getattr(gs, \"budget\", None) or getattr(gs, \"my_budget\", None)\n",
    "                state[\"current_item\"] = getattr(ca, \"item_name\", None) or getattr(gs, \"current_item\", None)\n",
    "                state[\"min_raise\"] = getattr(ca, \"min_raise\", None) or getattr(gs, \"min_raise\", None)\n",
    "                state[\"current_bid\"] = getattr(ca, \"current_bid\", None) or getattr(gs, \"current_bid\", None)\n",
    "                state[\"valuations\"] = getattr(gs, \"valuations\", None) or getattr(gs, \"my_valuations\", None)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # If anything missing, let’s try asking the tool explicitly (through LLM agent)\n",
    "        # while staying within max_steps=5 — this is a cheap “state refresh” prompt.\n",
    "        if any(state.get(k) is None for k in (\"budget\", \"current_bid\", \"min_raise\")):\n",
    "            try:\n",
    "                refresh = self.agent.run(\n",
    "                    \"Call get_game_state and return ONLY the raw JSON you receive.\"\n",
    "                )\n",
    "                tool_state = _extract_dict(refresh)\n",
    "                # merge tool_state into state\n",
    "                for k, v in tool_state.items():\n",
    "                    if state.get(k) is None and v is not None:\n",
    "                        state[k] = v\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # --------- 5) Validate / legalize LLM decision ----------\n",
    "        action = str(decision.get(\"action\", \"PASS\")).upper()\n",
    "        amount = decision.get(\"amount\", None)\n",
    "        legalized = _legalize(action, amount, state)\n",
    "        if legalized[0] == BidAction.BID:\n",
    "            return legalized\n",
    "\n",
    "        # --------- 6) Fallback heuristic (safe + decent) ----------\n",
    "        # Heuristic: keep a reserve for next item proportional to its value.\n",
    "        valuations = state.get(\"valuations\") or {}\n",
    "        current_item = state.get(\"current_item\")\n",
    "\n",
    "        budget = int(state.get(\"budget\", 0) or 0)\n",
    "        current_bid = int(state.get(\"current_bid\", 0) or 0)\n",
    "        min_raise = int(state.get(\"min_raise\", getattr(self.env, \"MIN_RAISE\", 5)) or getattr(self.env, \"MIN_RAISE\", 5))\n",
    "        min_bid = int(getattr(self.env, \"MIN_BID\", 3))\n",
    "\n",
    "        # Determine current value\n",
    "        v_cur = None\n",
    "        if isinstance(valuations, dict) and current_item in valuations:\n",
    "            v_cur = int(valuations[current_item])\n",
    "        else:\n",
    "            # try common item keys\n",
    "            for key in (\"Magic Book\", \"Flying Carpet\", \"Item A\", \"Item B\", \"A\", \"B\"):\n",
    "                if key == current_item and key in valuations:\n",
    "                    v_cur = int(valuations[key])\n",
    "                    break\n",
    "        if v_cur is None:\n",
    "            v_cur = 0\n",
    "\n",
    "        # Estimate next item reserve if this is item A / first round\n",
    "        # (this is intentionally simple and robust to different naming)\n",
    "        v_next = 0\n",
    "        if isinstance(valuations, dict):\n",
    "            if current_item in (\"Magic Book\", \"Item A\", \"A\"):\n",
    "                v_next = int(valuations.get(\"Flying Carpet\", valuations.get(\"Item B\", valuations.get(\"B\", 0))) or 0)\n",
    "\n",
    "        reserve = int(0.8 * v_next)  # save most of what you'd need for next item\n",
    "        max_pay = max(0, budget - reserve)\n",
    "\n",
    "        if current_bid <= 0:\n",
    "            # open with minimum if we actually care\n",
    "            if max_pay >= min_bid and v_cur > 0:\n",
    "                return (BidAction.BID, min_bid)\n",
    "            return (BidAction.PASS, None)\n",
    "\n",
    "        min_allowed = current_bid + min_raise\n",
    "        if min_allowed <= max_pay and v_cur > 0:\n",
    "            return (BidAction.BID, min_allowed)\n",
    "\n",
    "        return (BidAction.PASS, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# RUN A SIMPLE AUCTION\n",
    "# =========================\n",
    "\n",
    "valuations = {\n",
    "    \"Mickey\": {\"Magic Book\": 30, \"Flying Carpet\": 25},\n",
    "    \"Minnie\": {\"Magic Book\": 20, \"Flying Carpet\": 20},\n",
    "}\n",
    "\n",
    "env = MiniAuctionEnvironment([\"Mickey\", \"Minnie\"], valuations)\n",
    "\n",
    "mickey = MiniStrategicAgent(\"Mickey\", env, model=None)\n",
    "minnie = MiniStrategicAgent(\"Minnie\", env, model=None)\n",
    "\n",
    "agents = {\n",
    "    \"Mickey\": mickey,\n",
    "    \"Minnie\": minnie\n",
    "}\n",
    "\n",
    "turn_order = [\"Mickey\", \"Minnie\"]\n",
    "\n",
    "print(\"=== Auction Start: Magic Book ===\\n\")\n",
    "\n",
    "while len(env.game_state.current_auction.active_bidders) > 1:\n",
    "    for name in turn_order:\n",
    "        if name not in env.game_state.current_auction.active_bidders:\n",
    "            continue\n",
    "        action, amount = agents[name].decide_bid()\n",
    "        env.apply_action(name, action, amount)\n",
    "        if len(env.game_state.current_auction.active_bidders) == 1:\n",
    "            break\n",
    "\n",
    "winner = next(iter(env.game_state.current_auction.active_bidders))\n",
    "print(\"\\n=== Auction Result ===\")\n",
    "print(f\"Winner: {winner}\")\n",
    "print(f\"Final Price: {env.game_state.current_auction.current_bid}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_atc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
