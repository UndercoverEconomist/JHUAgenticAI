{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752ed9f2-0114-49cc-a2ab-fa78bede34b9",
   "metadata": {},
   "source": [
    "<div align=\"right\">&copy; Guven</div>\n",
    "\n",
    "<img src=\"../figs/ep.logo.png\" alt=\"JHU EP logo\" width=\"200\" align=\"right\" />\n",
    "\n",
    "# Uncertain Knowledge and Reasoning\n",
    "__Note:__ Handle the lecture **Sep 30**\n",
    "Topics:\n",
    "- Logical Foundations under Uncertainty\n",
    "  - Classical vs. non-classical logics (first-order, modal, temporal)\n",
    "  - Epistemic logic for knowledge and belief representation\n",
    "  - Common knowledge, distributed knowledge, and belief revision\n",
    "  - Limitations of purely deductive reasoning in uncertain environments\n",
    "- Probabilistic Reasoning\n",
    "  - Probability theory as a calculus for uncertainty\n",
    "  - Bayesian networks and influence diagrams\n",
    "  - Probabilistic graphical models for multiagent systems\n",
    "  - Trade-offs: tractability vs. expressivity&#x20;\n",
    "- Nonmonotonic and Default Reasoning\n",
    "  - Handling incomplete or defeasible information\n",
    "  - Default logic, circumscription, and autoepistemic logic\n",
    "  -  Reasoning with exceptions and revisable assumptions\n",
    "  - Connections to belief change and knowledge update\n",
    "- Decision-Theoretic Approaches\n",
    "  - Decision theory and utility functions under uncertainty\n",
    "  - Markov decision processes (MDPs) and partially observable MDPs (POMDPs)\n",
    "  - Bounded rationality and resource-constrained reasoning\n",
    "  - Applications in planning, robotics, and agent decision-making\n",
    "- Reasoning about Knowledge Dynamics\n",
    "  - Belief revision and update (AGM theory and beyond)\n",
    "  - Knowledge fusion, arbitration, and inconsistency management\n",
    "  - Temporal aspects of belief change in dynamic environments\n",
    "  - Logics of intention and commitment in uncertain domains\n",
    "\n",
    "- Knowledge Representation\n",
    "  - Formal Ontologies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4787a9e4-8e81-465e-acd2-da92628b7dff",
   "metadata": {},
   "source": [
    "## 6.1 Logical Foundations under Uncertainty\n",
    "### Classical vs. Non-Classical Logics\n",
    "**Classical logic** (propositional and first-order logic) provides a foundation for representing certain knowledge, but it assumes complete and consistent information.\n",
    "\n",
    "### Example 1:\n",
    "* Premises: `All humans are mortal`, `Socrates is a human`\n",
    "* Deduction: `Socrates is mortal`\n",
    "\n",
    "**Limitations** brittle under incomplete or inconsistent information. If we add `Socrates is not mortal`, classical logic becomes inconsistent and trivial (everything follows).\n",
    "\n",
    "**Non-classical logics** extend or relax assumptions,\n",
    "* **Modal logic** adds modalities like $\\square\\phi$ (necessarily $\\phi$, always $\\phi$) and $\\lozenge \\phi$ (possibly $\\phi$, sometimes $\\phi$). These are central for modeling knowledge ($K_i \\, \\phi =$ agent $i$ knows $\\phi$), belief ($B_i \\, \\phi =$ agent $i$ believes $\\phi$), and obligation ($O \\, \\phi =$ it ought to be the case that $\\phi$)\n",
    "* **Temporal logic** adds time operators like $\\mathsf{G}$ (always), $\\mathsf{F}$ (eventually). Example: \"Eventually the system will recover from failure.\"\n",
    "* **Nonmonotonic logics** allow retraction of conclusions when new evidence arrives.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Medical Diagnosis with Uncertainty\n",
    "#### Pathway 1. Classical Deduction\n",
    "Knowledge base (KB):\n",
    "1. $\\forall x \\; ( \\text{Flu}(x) \\rightarrow \\text{Fever}(x) )$\n",
    "  - If someone has flu, then they have a fever\n",
    "2. $\\forall x \\; ( \\text{COVID}(x) \\rightarrow \\text{Fever}(x) \\wedge \\text{Cough}(x)) $\n",
    "  - If someone has COVID, then they have fever and cough\n",
    "3. $\\text{COVID}(\\text{Alice})$\n",
    "\n",
    "Deduction (via modus ponens):  $\\text{Fever}(\\text{Alice}) \\wedge \\text{Cough}(\\text{Alice})$\n",
    "\n",
    "So, Alice has fever and cough.\n",
    "\n",
    "#### Pathway 2. Contradiction in Classical Logic\n",
    "Suppose we also record:\n",
    "$\\lnot \\text{Fever}(\\text{Alice})$\n",
    "\n",
    "Now KB is inconsistent. By the **principle of explosion**, we could derive anything:\n",
    "\n",
    "$\\{ \\text{COVID}(\\text{Alice}), \\; \\lnot \\text{Fever}(\\text{Alice}) \\} \\vdash \\text{MoonIsCheese}$\n",
    "\n",
    "$\\longrightarrow$ Classical logic becomes unusable in the face of contradictions.\n",
    "\n",
    "#### Pathway 3. Modal Extension (Knowledge & Belief)\n",
    "Introduce agents: **Doctor (D)** and **Patient (P)**.\n",
    "\n",
    "* \\$K\\_D (\\text{COVID}(\\text{Alice}))\\$ = *The doctor knows Alice has COVID.*\n",
    "* \\$B\\_P (\\lnot \\text{COVID}(\\text{Alice}))\\$ = *Alice believes she does not have COVID.*\n",
    "\n",
    "This highlights the difference between **objective facts** vs. **agents' perspectives**.\n",
    "\n",
    "#### Pathway 4. Temporal Extension (Disease Progression)\n",
    "\n",
    "We can model Alice‚Äôs recovery timeline:\n",
    "\n",
    "* \\$\\mathsf{G} ; \\text{Fever}(\\text{Alice})\\$ ‚Üí *Always, Alice has fever* (chronic illness).\n",
    "* \\$\\mathsf{F} ; \\lnot \\text{Fever}(\\text{Alice})\\$ ‚Üí *Eventually, Alice will no longer have fever* (recovery).\n",
    "\n",
    "If the system encodes:\n",
    "$\\text{Treatment}(\\text{Alice}) \\rightarrow \\mathsf{F} \\; \\lnot \\text{Fever}(\\text{Alice})$\n",
    "then giving treatment ensures that eventually Alice recovers.\n",
    "\n",
    "#### Pathway 5. Nonmonotonic Extension (Default Reasoning)\n",
    "Default rules:\n",
    "* $\\text{Bird}(x) : \\text{Fly}(x)$ ‚Äî *Birds typically fly*\n",
    "* $\\text{Patient}(x) : \\lnot \\text{COVID}(x)$ ‚Äî *Patients typically do not have COVID*\n",
    "\n",
    "So, initially we infer:\n",
    "$\\lnot \\text{COVID}(\\text{Alice})$\n",
    "\n",
    "But once we observe \\$\\text{COVID}(\\text{Alice})\\$ explicitly, we **retract** the default inference.\n",
    "\n",
    "Similarly, if we add:\n",
    "$\\text{Vaccinated}(\\text{Alice}) \\rightarrow \\lnot \\text{Severe}(\\text{COVID}(\\text{Alice}))$\n",
    "we can update Alice‚Äôs prognosis while keeping default rules for other patients.\n",
    "\n",
    "#### Pathway 6. Integrated Reasoning\n",
    "Putting it all together:\n",
    "* **Classical**: ensures deductive structure of symptoms.\n",
    "* **Modal**: distinguishes what the doctor *knows* vs. what Alice *believes*.\n",
    "* **Temporal**: models how symptoms and recovery evolve over time.\n",
    "* **Nonmonotonic**: handles defaults like *‚Äúpatients usually don‚Äôt have COVID‚Äù* and allows retraction when exceptions arise.\n",
    "\n",
    "---\n",
    "\n",
    "üìö **References**\n",
    "\n",
    "* Wooldridge, M. (2002). *Multiagent Systems*, Ch. 12 (logics for knowledge, belief, time).\n",
    "* Shoham, Y., & Leyton-Brown, K. (2009). *Multiagent Systems*, Ch. 13‚Äì14 (epistemic, temporal, belief revision).\n",
    "* Alchourr√≥n, C. E., G√§rdenfors, P., & Makinson, D. (1985). *On the Logic of Theory Change*. J. Symbolic Logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb20e0-3d56-4c92-a990-aa849b21845c",
   "metadata": {},
   "source": [
    "## 6.2 Epistemic Logic for Knowledge and Belief Representation\n",
    "\n",
    "**Epistemic logic** (modal logic of knowledge) models what agents know or believe.\n",
    "\n",
    "* Syntax: `Ki œÜ` means *agent i knows œÜ*.\n",
    "* Semantics: possible worlds and accessibility relations.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "* In a security protocol, suppose `K_A (p)` = \"Agent A knows the password p\".\n",
    "* If `K_B (K_A (p))`, then B knows that A knows the password (second-order knowledge).\n",
    "\n",
    "**Belief vs. Knowledge**:\n",
    "\n",
    "* Knowledge is modeled as truthful (`Ki œÜ ‚Üí œÜ`).\n",
    "* Belief allows falsehood but is consistent (`Bi œÜ ‚Üí ¬¨ Bi (¬¨œÜ)`).\n",
    "\n",
    "Applications: reasoning about what agents know in distributed systems (e.g., distributed consensus, fault-tolerant protocols).\n",
    "\n",
    "*Reference:* Shoham & Leyton-Brown (*Multiagent Systems*, Ch. 13) formalize epistemic logic with S5 axioms. See also Halpern (1995) *Reasoning about Knowledge*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b3393a-75ad-40b8-9597-5ad83efdc166",
   "metadata": {},
   "source": [
    "\n",
    "## 6.3 Common Knowledge, Distributed Knowledge, and Belief Revision\n",
    "\n",
    "### Common Knowledge\n",
    "\n",
    "A proposition œÜ is **common knowledge** in a group if everyone knows œÜ, everyone knows that everyone knows œÜ, and so on ad infinitum.\n",
    "\n",
    "* Example: In a meeting, if someone announces \"The fire alarm is ringing,\" and everyone hears it, this becomes common knowledge.\n",
    "\n",
    "**Importance**: coordination in distributed systems (e.g., agreement in multiagent planning).\n",
    "\n",
    "### Distributed Knowledge\n",
    "\n",
    "Group knowledge may exceed any individual‚Äôs.\n",
    "\n",
    "* Example: Agent A knows *if it rains the ground is wet*, and B knows *it rains*. Together, the group knows *the ground is wet*.\n",
    "\n",
    "### Belief Revision\n",
    "\n",
    "When new evidence contradicts old beliefs, agents revise beliefs using rationality postulates (AGM theory).\n",
    "\n",
    "* Example:\n",
    "\n",
    "  * Initial beliefs: `{All birds fly}`\n",
    "  * New info: `Penguins are birds that do not fly`\n",
    "  * Revision: retract universal rule, replace with default: `{Birds typically fly}`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13488b8d-c0a9-4351-b60c-dd3e1a065910",
   "metadata": {},
   "source": [
    "## 6.4 Limitations of Purely Deductive Reasoning\n",
    "\n",
    "Purely deductive systems assume:\n",
    "\n",
    "* **Consistency**: no contradictions.\n",
    "* **Closure**: all logical consequences can be derived.\n",
    "* **Omniscience**: agents know all consequences of their knowledge.\n",
    "\n",
    "**Problems in uncertain environments**:\n",
    "\n",
    "* **Resource bounds**: real agents can‚Äôt compute all consequences (bounded rationality).\n",
    "* **Incomplete information**: classical logic cannot rank competing hypotheses.\n",
    "* **Example**: In medical diagnosis, deductive rules may conclude both \"disease A\" and \"not disease A\" if symptoms are ambiguous. Probabilistic or default reasoning is needed.\n",
    "\n",
    "Wooldridge (2002) emphasizes that logical deduction alone is too rigid for dynamic, uncertain, multiagent settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bab8c6-bcb0-4716-827a-82c72256e82e",
   "metadata": {},
   "source": [
    "## 6.5 Probabilistic Reasoning\n",
    "\n",
    "Classical logic = brittle under uncertainty (all-or-nothing truth).\n",
    "\n",
    "Probabilistic reasoning = models degrees of belief.\n",
    "\n",
    "Probability theory = a calculus for uncertainty:\n",
    "* Events ‚Üí outcomes of experiments.\n",
    "* Probabilities $\\in [0,1]$ quantify uncertainty.\n",
    "\n",
    "** Example ** Weather forecast:\n",
    "$P(\\text{Rain}) = 0.3$, $P(\\lnot \\text{Rain}) = 0.7$\n",
    "More informative than saying only \"it will rain or not.\"\n",
    "\n",
    "### Axioms (Kolmogorov):\n",
    "\n",
    "$0 \\leq P(A) \\leq 1$\n",
    "\n",
    "$P(\\Omega) = 1$\n",
    "\n",
    "$P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$\n",
    "\n",
    "Conditional probability:\n",
    "\n",
    "$P(A‚à£B)=\\frac{P(A‚à©B)}{P(B)}$\n",
    "\n",
    "Bayes' Rule:\n",
    "\n",
    "$P(H‚à£E)=\\frac{P(E‚à£H)P(H)}{P(E)}$\n",
    "\n",
    "**Example** Medical diagnosis:\n",
    "\n",
    "Prior: $P(\\text{Disease}) = 0.01$\n",
    "\n",
    "Likelihood: $P(\\text{Test+} \\mid \\text{Disease}) = 0.9$\n",
    "\n",
    "Bayes updates belief in disease after test result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f15a4b-9dfe-478d-83b4-02234ed2d728",
   "metadata": {},
   "source": [
    "## 6.6 Bayesian Networks\n",
    "\n",
    "Bayesian Network (BN): Directed acyclic graph (DAG) with:\n",
    "* Nodes = random variables.\n",
    "* Edges = dependencies.\n",
    "* CPTs (conditional probability tables) at each node.\n",
    "\n",
    "Factorizes joint distribution:\n",
    "\n",
    "$P(X_1,\\dots,X_n) = \\prod_i P(X_i \\mid \\text{Parents}(X_i))$\n",
    "\n",
    "**Example BN**\n",
    "\n",
    "$\\text{Rain} \\rightarrow \\text{Sprinkler} \\rightarrow \\text{WetGrass}$\n",
    "\n",
    "Captures that wet grass depends on rain and sprinkler.\n",
    "\n",
    "### Influence Diagrams\n",
    "Extension of BNs for decision making.\n",
    "\n",
    "Components:\n",
    "- Chance nodes (uncertainty).\n",
    "- Decision nodes (actions).\n",
    "- Utility nodes (preferences).\n",
    "\n",
    "Supports rational choice under uncertainty.\n",
    "\n",
    "**Example** Medical influence diagram:\n",
    "- Chance: Disease, Test Result.\n",
    "- Decision: Treat or not.\n",
    "- Utility: Survival, side effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a082dc-d515-45d4-b666-b3b7c10b8167",
   "metadata": {},
   "source": [
    "## 6.7 Probabilistic Graphical Models in Multiagent Systems\n",
    "Multiagent environments = uncertainty about:\n",
    "- State of world.\n",
    "- Actions/intentions of other agents.\n",
    "\n",
    "Graphical models allow compact representation:\n",
    "- Multiagent Influence Diagrams (MAIDs).\n",
    "- Dynamic Bayesian Networks for evolving systems.\n",
    "\n",
    "**Example** Autonomous cars negotiating intersection.\n",
    "- Each car models probability of others stopping/going.\n",
    "\n",
    "### Trade-offs: Tractability vs. Expressivity\n",
    "- Expressive models (rich dependencies) = accurate but often intractable (NP-hard inference).\n",
    "- Simplified models (e.g., na√Øve Bayes) = tractable but oversimplify dependencies.\n",
    "- Approximate inference:\n",
    "  - Sampling (Monte Carlo, MCMC).\n",
    "  - Variational methods.\n",
    "\n",
    "#### Key tension\n",
    "- More expressive ‚Üí harder to compute.\n",
    "- More tractable ‚Üí less faithful to reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03815d50-3eea-499a-800a-fec5c5f4159e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.8 Nonmonotonic Reasoning\n",
    "\n",
    "* **Monotonicity (classical logic):**\n",
    "  If $\\Gamma \\vdash \\varphi$, then for any $\\psi$,\n",
    "  $\\Gamma \\cup {\\psi} \\vdash \\varphi$\n",
    "* Means: once something is derived, it **cannot be retracted**, even if new info arrives.\n",
    "\n",
    "**Problem:** Real-world reasoning is **defeasible**.\n",
    "\n",
    "* Example:\n",
    "  $\\text{Bird}(x) \\rightarrow \\text{Flies}(x)$\n",
    "  Then add:\n",
    "  $\\text{Penguin}(x) \\rightarrow \\text{Bird}(x) \\wedge \\neg \\text{Flies}(x)$\n",
    "* Must retract the default inference for penguins.\n",
    "\n",
    "### Default Logic (Reiter, 1980)\n",
    "\n",
    "* A **default theory** is a pair:\n",
    "  $\\Delta = (W, D)$\n",
    "  where:\n",
    "\n",
    "  * $W$ = set of facts (first-order sentences).\n",
    "  * $D$ = set of default rules.\n",
    "\n",
    "* A **default rule**:\n",
    "  $\\dfrac{\\alpha : \\beta}{\\gamma}$\n",
    "  Read as: if $\\alpha$ is provable, and $\\beta$ is consistent, infer $\\gamma$.\n",
    "\n",
    "**Example:** Birds typically fly.\n",
    "$\\dfrac{\\text{Bird}(x) : \\text{Flies}(x)}{\\text{Flies}(x)}$\n",
    "\n",
    "* If $\\text{Bird}(\\text{Tweety}) \\in W$, then we infer $\\text{Flies}(\\text{Tweety})$, unless contradicted.\n",
    "\n",
    "### Circumscription (McCarthy, 1980)\n",
    "\n",
    "* **Idea:** Assume what is not known to be true is false (*minimize predicates*).\n",
    "* Formalized as **predicate circumscription**:\n",
    "  $\\text{Circ}[P; Q_1, \\ldots, Q_n; R_1, \\ldots, R_m](\\varphi)$\n",
    "  means: minimize $P$, vary $Q_i$, fix $R_j$.\n",
    "\n",
    "**Example:** Only known birds are birds.\n",
    "\n",
    "* Knowledge:\n",
    "  $\\text{Bird}(\\text{Tweety}), ; \\text{Bird}(\\text{Polly})$\n",
    "* Circumscription minimizes $\\text{Bird}$, so we infer **no other birds exist** unless evidence is added.\n",
    "\n",
    "###  Autoepistemic Logic (Moore, 1985)\n",
    "\n",
    "* Models an agent‚Äôs **reasoning about its own beliefs**.\n",
    "* Introduces modal operator $L$:\n",
    "\n",
    "  * $L \\varphi =$ ‚Äú$\\varphi$ is believed.‚Äù\n",
    "  * $\\neg L \\varphi =$ ‚Äú$\\varphi$ is not believed.‚Äù\n",
    "\n",
    "**Example:** ‚ÄúIf I cannot prove not-flying, I assume flying.‚Äù\n",
    "$\\text{Bird}(x) \\rightarrow \\text{Flies}(x) ; \\lor ; L\\neg\\text{Flies}(x)$\n",
    "\n",
    "So if $\\text{Bird}(\\text{Tweety})$ is known and $\\neg L\\neg\\text{Flies}(\\text{Tweety})$, infer $\\text{Flies}(\\text{Tweety})$.\n",
    "\n",
    "###  Reasoning with Exceptions\n",
    "\n",
    "* **Penguin example in default logic**:\n",
    "\n",
    "  * Defaults:\n",
    "    $\\dfrac{\\text{Bird}(x) : \\text{Flies}(x)}{\\text{Flies}(x)}$\n",
    "    $\\dfrac{\\text{Penguin}(x) : \\neg \\text{Flies}(x)}{\\neg \\text{Flies}(x)}$\n",
    "\n",
    "* Facts:\n",
    "  $\\text{Penguin}(\\text{Tweety})$\n",
    "\n",
    "* Derivation:\n",
    "\n",
    "  * From $\\text{Penguin}(\\text{Tweety})$ infer $\\text{Bird}(\\text{Tweety})$.\n",
    "  * Default 1 suggests $\\text{Flies}(\\text{Tweety})$.\n",
    "  * Default 2 blocks it $\\Rightarrow$ final extension contains $\\neg \\text{Flies}(\\text{Tweety})$.\n",
    "\n",
    "### Belief Revision and Updates\n",
    "\n",
    "* **AGM Postulates** (Alchourr√≥n, G√§rdenfors, Makinson, 1985):\n",
    "  A belief set $K$ closed under consequence ($Cn$).\n",
    "\n",
    "  * **Expansion:** $K + \\varphi = Cn(K \\cup {\\varphi})$\n",
    "  * **Contraction:** $K - \\varphi$ removes $\\varphi$ while preserving consistency.\n",
    "  * **Revision:** $K * \\varphi$ = add $\\varphi$ and ensure consistency.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "1. $K = {\\text{Bird}(\\text{Tweety}) \\rightarrow \\text{Flies}(\\text{Tweety}), ; \\text{Bird}(\\text{Tweety})}$\n",
    "2. Infer: $\\text{Flies}(\\text{Tweety})$\n",
    "3. Add $\\neg \\text{Flies}(\\text{Tweety})$ $\\Rightarrow$ revise $K$ to drop the universal rule.\n",
    "\n",
    "### Connections Across Frameworks\n",
    "\n",
    "* **Default logic** = ‚Äúif consistent, assume.‚Äù\n",
    "* **Circumscription** = ‚Äúminimize what‚Äôs unknown.‚Äù\n",
    "* **Autoepistemic logic** = ‚Äúreason about one‚Äôs own ignorance.‚Äù\n",
    "* **AGM theory** = principled belief change.\n",
    "\n",
    "**Common goal:**\n",
    "Handle **incomplete, defeasible, and revisable** information while avoiding the collapse of classical logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa621d7-8bdc-404e-a2ea-e0402b2da458",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.9 Decision Theory under Uncertainty\n",
    "* **Core idea:** Rational agents should act to maximize **expected utility (EU)**\n",
    "\n",
    "* A decision problem is represented by:\n",
    "  * A set of states $S$\n",
    "  * A set of actions $A$\n",
    "  * A utility function $U: S \\rightarrow \\mathbb{R}$\n",
    "  * A probability distribution $P(s)$ over states\n",
    "* **Expected utility of an action $a$:**\n",
    "  $EU(a) = \\sum_{s \\in S} P(s \\mid a) \\cdot U(s)$\n",
    "\n",
    "**Example:**\n",
    "* Two actions: *Carry umbrella* $(a_1)$, *Don‚Äôt carry umbrella* $(a_2)$\n",
    "* $P(\\text{Rain}) = 0.3$, $U(\\text{Dry})=+10$, $U(\\text{Wet})=-20$\n",
    "* Agent chooses $a$ with maximal $EU(a)$.\n",
    "\n",
    "### Markov Decision Processes (MDPs)\n",
    "* An **MDP** is a tuple:\n",
    "  $\\langle S, A, T, R, \\gamma \\rangle$\n",
    "\n",
    "  where:\n",
    "  * $S$: states\n",
    "  * $A$: actions\n",
    "  * $T(s,a,s') = P(s' \\mid s,a)$: transition probabilities\n",
    "  * $R(s,a)$: reward for taking $a$ in $s$\n",
    "  * $\\gamma \\in [0,1)$: discount factor\n",
    "\n",
    "* **Objective:** Find a policy $\\pi: S \\rightarrow A$ maximizing expected return.\n",
    "\n",
    "* **Value function:**\n",
    "  $V^\\pi(s) = \\mathbb{E}\\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, \\pi(s_t)) ,\\Big|, s_0 = s \\right]$\n",
    "\n",
    "* **Bellman optimality equation:**\n",
    "  $V^*(s) = \\max_a \\Big[ R(s,a) + \\gamma \\sum_{s'} T(s,a,s') , V^*(s') \\Big]$\n",
    "\n",
    "### POMDPs (Partially Observable MDPs)\n",
    "* In real domains, state is **not fully observable**.\n",
    "* A **POMDP** extends MDPs with:\n",
    "  * Observation set $O$\n",
    "  * Observation function $Z(s',a,o) = P(o \\mid s',a)$\n",
    "\n",
    "* Agent maintains a **belief state** $b(s)$ = probability distribution over states.\n",
    "\n",
    "* **Belief update (Bayes filter):**\n",
    "  $b'(s') = \\eta , Z(s',a,o) \\sum_{s \\in S} T(s,a,s') , b(s)$\n",
    "  where $\\eta$ is a normalizing constant.\n",
    "\n",
    "* Optimal policy maps **belief states** $\\pi: b \\mapsto a$.\n",
    "\n",
    "### Bounded Rationality\n",
    "* Classical decision theory assumes unlimited computation.\n",
    "\n",
    "* **Bounded rationality** (Herbert Simon):\n",
    "  Agents maximize utility **within resource limits**.\n",
    "\n",
    "* **Approximate methods:**\n",
    "  * Limited-horizon planning in MDPs.\n",
    "  * Heuristic utility functions.\n",
    "  * Sampling-based approximations (Monte Carlo).\n",
    "\n",
    "**Formal view:**\n",
    "If $C(a)$ = computational cost of action $a$, then effective utility:\n",
    "\n",
    "$U'(a) = U(a) - \\lambda , C(a)$\n",
    "\n",
    "where $\\lambda$ balances performance vs. cost.\n",
    "\n",
    "### Applications\n",
    "1. **Planning**:\n",
    "   * Probabilistic planning as solving MDPs.\n",
    "   * Example: Robot navigation with stochastic motion.\n",
    "2. **Robotics**:\n",
    "   * POMDPs model noisy sensors + uncertain actuation.\n",
    "   * Example: Autonomous drone with partial visibility.\n",
    "3. **Multiagent Systems**:\n",
    "   * Decision-theoretic approaches underpin auctions, negotiation, coordination.\n",
    "   * Game-theoretic equilibria often computed as *best-response policies*.\n",
    "4. **Everyday AI**:\n",
    "   * Medical decision support: choosing tests/treatments with uncertain outcomes.\n",
    "   * Self-driving cars: optimizing routes and collision-avoidance under uncertainty.\n",
    "\n",
    "\n",
    "Would you like me to also make a **worked numerical example** (step-by-step Bellman updates on a tiny gridworld MDP) so students can *see* how value iteration converges?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887aa667-9d49-403a-9c45-0281534b51f4",
   "metadata": {},
   "source": [
    "## Example Problem Setup (tiny gridworld)\n",
    "* Grid (row, col). Terminals are absorbing with fixed values:\n",
    "  * $(1,3)$ is a **goal** with $+1$\n",
    "  * $(3,3)$ is a **pit** with $-1$\n",
    "\n",
    "* All other cells have **living reward** $R(s) = -0.04$ per step.\n",
    "\n",
    "* **Actions**: ${\\text{N},\\text{S},\\text{E},\\text{W}}$, deterministic.\n",
    "\n",
    "* Hitting a wall keeps you in place.\n",
    "\n",
    "* **Discount**: $\\gamma = 1.0$\n",
    "\n",
    "* **Bellman optimality update** (deterministic, state-reward form):\n",
    "\n",
    "$\n",
    "V_{k+1}(s) =\n",
    "\\begin{cases}\n",
    "+1 & \\text{if } s=(1,3) \\\\\n",
    "-1 & \\text{if } s=(3,3) \\\\\n",
    "R(s) + \\gamma \\cdot \\max\\limits_{a} V_k!\\big(s'=\\delta(s,a)\\big) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where $\\delta(s,a)$ is the next state from $s$ taking action $a$.\n",
    "\n",
    "* **Initialization**: $V_0(s)=0$ for nonterminal states; $V_0(1,3)=+1$, $V_0(3,3)=-1$.\n",
    "\n",
    "We present values as a table per iteration (top row is row 1):\n",
    "* Row 1: $(1,1), (1,2), (1,3=+1)$\n",
    "* Row 2: $(2,1), (2,2), (2,3)$\n",
    "* Row 3: $(3,1), (3,2), (3,3=-1)$\n",
    "\n",
    "\n",
    "## Iteration $k=0$ (initial values)\n",
    "$\n",
    "\\begin{array}{ccc}\n",
    "0 & 0 & \\color{blue}{+1} \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & \\color{blue}{-1}\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "## Iteration $k=1$\n",
    "For nonterminal $s$: $V_1(s) = -0.04 + \\max_a V_0(s')$.\n",
    "\n",
    "A couple of explicit updates:\n",
    "* $(1,2)$ neighbors (N,S,W,E): $(1,2)$, $(2,2)$, $(1,1)$, $(1,3)$ with $V_0={0,0,0,+1}$\n",
    "  $\n",
    "  V_1(1,2) = -0.04 + \\max{0,0,0,1} = 0.96\n",
    "  $\n",
    "* $(2,3)$ neighbors (N,S,W,E): $(1,3)$, $(3,3)$, $(2,2)$, $(2,3)$ with $V_0={+1,-1,0,0}$\n",
    "  $\n",
    "  V_1(2,3) = -0.04 + \\max{1,-1,0,0} = 0.96\n",
    "  $\n",
    "\n",
    "All others that don‚Äôt touch a terminal pick up $\\max=0$:\n",
    "$\n",
    "V_1(s) = -0.04\n",
    "$\n",
    "\n",
    "Table:\n",
    "\n",
    "$\n",
    "\\begin{array}{ccc}\n",
    "-0.04 & \\mathbf{0.96} & \\color{blue}{+1} \\\\\n",
    "-0.04 & -0.04 & \\mathbf{0.96} \\\\\n",
    "-0.04 & -0.04 & \\color{blue}{-1}\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "## Iteration $k=2$\n",
    "Use $V_1$ on the right-hand side (**synchronous** updates).\n",
    "\n",
    "Examples:\n",
    "* $(2,2)$ neighbors $(1,2),(3,2),(2,1),(2,3)$ with $V_1={0.96,-0.04,-0.04,0.96}$:\n",
    "  $\n",
    "  V_2(2,2) = -0.04 + \\max{0.96,-0.04,-0.04,0.96} = 0.92\n",
    "  $\n",
    "* $(1,1)$ neighbors $(1,1),(2,1),(1,1),(1,2)$ with $V_1={-0.04,-0.04,-0.04,0.96}$:\n",
    "  $\n",
    "  V_2(1,1) = -0.04 + 0.96 = 0.92\n",
    "  $\n",
    "\n",
    "Cells next to terminals remain $0.96$; farther cells improve slightly.\n",
    "\n",
    "Table:\n",
    "\n",
    "$\n",
    "\\begin{array}{ccc}\n",
    "\\mathbf{0.92} & \\mathbf{0.96} & \\color{blue}{+1} \\\\\n",
    "-0.08 & \\mathbf{0.92} & \\mathbf{0.96} \\\\\n",
    "-0.08 & -0.08 & \\color{blue}{-1}\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "## Iteration $k=3$\n",
    "Use $V_2$.\n",
    "\n",
    "Examples:\n",
    "* $(3,2)$ neighbors $(2,2),(3,2),(3,1),(3,3)$ with $V_2={0.92,-0.08,-0.08,-1}$:\n",
    "  $\n",
    "  V_3(3,2) = -0.04 + \\max{0.92,-0.08,-0.08,-1} = 0.88\n",
    "  $\n",
    "* $(2,1)$ neighbors $(1,1),(3,1),(2,1),(2,2)$ with $V_2={0.92,-0.08,-0.08,0.92}$:\n",
    "  $\n",
    "  V_3(2,1) = -0.04 + 0.92 = 0.88\n",
    "  $\n",
    "\n",
    "Table:\n",
    "\n",
    "$\n",
    "\\begin{array}{ccc}\n",
    "\\mathbf{0.92} & \\mathbf{0.96} & \\color{blue}{+1} \\\\\n",
    "\\mathbf{0.88} & \\mathbf{0.92} & \\mathbf{0.96} \\\\\n",
    "-0.12 & \\mathbf{0.88} & \\color{blue}{-1}\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "## Iteration $k=4$\n",
    "Use $V_3$.\n",
    "\n",
    "* $(3,1)$ neighbors $(2,1),(3,1),(3,1),(3,2)$ with $V_3={0.88,-0.12,-0.12,0.88}$:\n",
    "  $\n",
    "  V_4(3,1) = -0.04 + 0.88 = 0.84\n",
    "  $\n",
    "  All other entries remain the same as iteration $3$.\n",
    "\n",
    "Table:\n",
    "\n",
    "$\n",
    "\\begin{array}{ccc}\n",
    "\\mathbf{0.92} & \\mathbf{0.96} & \\color{blue}{+1} \\\\\n",
    "\\mathbf{0.88} & \\mathbf{0.92} & \\mathbf{0.96} \\\\\n",
    "\\mathbf{0.84} & \\mathbf{0.88} & \\color{blue}{-1}\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "## Iteration $k=5$\n",
    "Using $V_4$, all values remain unchanged (fixed point reached).\n",
    "\n",
    "**Converged values** (optimal $V^*$):\n",
    "\n",
    "$\n",
    "\\begin{array}{ccc}\n",
    "0.92 & 0.96 & \\color{blue}{+1} \\\\\n",
    "0.88 & 0.92 & 0.96 \\\\\n",
    "0.84 & 0.88 & \\color{blue}{-1}\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "\n",
    "## Extracting the Optimal Policy $\\pi^*$\n",
    "Greedy w.r.t. $V^*$: $\\pi^*(s) = \\arg\\max_a V^*(\\delta(s,a))$ (for nonterminal $s$).\n",
    "\n",
    "Arrows indicate the best move (toward higher value):\n",
    "* Row 1: $\\rightarrow$ from $(1,1)$ to $(1,2)$; $\\rightarrow$ from $(1,2)$ to $(1,3)$ (goal)\n",
    "* Row 2: $(2,1)\\rightarrow(2,2)$; $(2,2)\\rightarrow(2,3)$; $(2,3)\\rightarrow(1,3)$ (up)\n",
    "* Row 3: $(3,1)\\rightarrow(3,2)$; $(3,2)\\rightarrow(2,2)$; $(3,3)$ is terminal\n",
    "\n",
    "In arrows:\n",
    "\n",
    "$\n",
    "\\begin{array}{ccc}\n",
    "\\rightarrow & \\rightarrow & \\color{blue}{\\text{GOAL}} \\\\\n",
    "\\rightarrow & \\rightarrow & \\uparrow \\\\\n",
    "\\rightarrow & \\uparrow & \\color{blue}{\\text{PIT}}\n",
    "\\end{array}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f936ef-b46a-43c3-b120-318e20289eea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.10 Knowledge Dynamics: Motivation\n",
    "* Agents often operate in **open, dynamic environments**.\n",
    "* Knowledge must be **updated, revised, or fused** as new information arrives.\n",
    "* Challenges:\n",
    "  * Inconsistencies\n",
    "  * Multiple sources of information\n",
    "  * Time-dependent beliefs\n",
    "  * Commitments and intentions over changing states\n",
    "\n",
    "### Belief Revision (AGM Theory)\n",
    "* A belief set $K$ is **closed under logical consequence** ($Cn$).\n",
    "* Three operations (AGM, 1985):\n",
    "  * **Expansion:** add new belief (no consistency guarantee).\n",
    "    $K + \\varphi = Cn(K \\cup {\\varphi})$\n",
    "  * **Contraction:** remove a belief to restore consistency.\n",
    "    $K - \\varphi$\n",
    "  * **Revision:** incorporate $\\varphi$ consistently.\n",
    "    $K * \\varphi = (K - \\neg \\varphi) + \\varphi$\n",
    "\n",
    "**Example:**\n",
    "* $K = {\\text{Bird}(\\text{Tweety}) \\rightarrow \\text{Flies}(\\text{Tweety}), ; \\text{Bird}(\\text{Tweety})}$\n",
    "* Infer $\\text{Flies}(\\text{Tweety})$.\n",
    "* Add $\\neg \\text{Flies}(\\text{Tweety})$ $;\\Rightarrow;$ revision drops universal rule.\n",
    "\n",
    "### Beyond AGM: Belief Update\n",
    "* **Revision**: world is static; new evidence corrects mistaken beliefs.\n",
    "* **Update**: world is dynamic; new evidence reflects change in reality.\n",
    "\n",
    "**Formal schema (Katsuno‚ÄìMendelzon, 1991):**\n",
    "* Revision postulates (AGM) focus on **consistency**.\n",
    "* Update postulates:\n",
    "\n",
    "* If $\\varphi$ holds in new world $w'$, then $K \\diamond \\varphi$ selects worlds ‚Äúclosest‚Äù to old $w$ where $\\varphi$ holds.\n",
    "\n",
    "**Example:**\n",
    "* Revision: \"Tweety doesn‚Äôt fly\" $;\\Rightarrow;$ drop default rule.\n",
    "* Update: \"Tweety broke wing\" $;\\Rightarrow;$ new world state causes non-flying.\n",
    "\n",
    "### Knowledge Fusion and Arbitration\n",
    "* Multiple sources ${K_1, K_2, \\dots, K_n}$ may provide conflicting beliefs.\n",
    "* **Fusion:** combine all into $K_F$.\n",
    "* **Arbitration:** balance between conflicting sources.\n",
    "\n",
    "**Operator view:**\n",
    "* Fusion operator $\\oplus$:\n",
    "  $K_F = K_1 \\oplus K_2 \\oplus \\cdots \\oplus K_n$\n",
    "* Arbitration chooses compromise model rather than intersection.\n",
    "\n",
    "**Example:**\n",
    "* $K_1$: ‚ÄúSensor says door is open.‚Äù\n",
    "* $K_2$: ‚ÄúSensor says door is closed.‚Äù\n",
    "* Arbitration may assign probabilities or confidence levels.\n",
    "\n",
    "### Inconsistency Management\n",
    "* Classical logic: if $K \\vdash \\bot$, then $K$ is **explosive** (everything derivable -- $\\bot$ is contradiction).\n",
    "* Solutions:\n",
    "  * **Paraconsistent logics**: allow $K$ to contain $\\varphi$ and $\\neg \\varphi$ without triviality.\n",
    "  * **Belief base repair**: remove minimal inconsistent subsets.\n",
    "\n",
    "**Formal view:**\n",
    "* Minimal inconsistent subset $MIS \\subseteq K$\n",
    "* Remove $MIS$ to restore consistency.\n",
    "\n",
    "\n",
    "### Temporal Belief Change\n",
    "* Beliefs evolve with time:\n",
    "  $K_t \\xrightarrow{;\\text{update at time }t;} K_{t+1}$\n",
    "\n",
    "* **Temporal logics** capture dynamics:\n",
    "  * $G\\varphi$ (‚Äúalways $\\varphi$‚Äù)\n",
    "  * $F\\varphi$ (‚Äúeventually $\\varphi$‚Äù)\n",
    "  * $X\\varphi$ (‚Äúnext state $\\varphi$‚Äù)\n",
    "  * $\\varphi U \\psi$ (‚Äú$\\varphi$ until $\\psi$‚Äù)\n",
    "\n",
    "**Example:**\n",
    "* $F(\\text{Recovered}(\\text{Alice}))$\n",
    "  = Eventually Alice recovers.\n",
    "* Belief revision adjusts prognosis as new medical test results arrive.\n",
    "\n",
    "\n",
    "### Intentions and Commitments\n",
    "* Beyond knowledge and belief:\n",
    "  * **Intention:** chosen course of action.\n",
    "  * **Commitment:** persistence of intention until fulfilled or dropped.\n",
    "\n",
    "* **Modal operators:**\n",
    "  * $I_a \\varphi$ = agent $a$ intends $\\varphi$.\n",
    "  * $C_a \\varphi$ = agent $a$ is committed to $\\varphi$.\n",
    "\n",
    "**Dynamic aspect:**\n",
    "* Commitments can be revised when:\n",
    "  * $\\varphi$ achieved.\n",
    "  * $\\varphi$ impossible.\n",
    "  * Higher-priority intention overrides.\n",
    "\n",
    "**Example:**\n",
    "* $I_{\\text{Robot}}(\\text{DeliverPackage})$\n",
    "* If obstacle arises, commitment persists until re-planning proves impossible.\n",
    "\n",
    "\n",
    "### Summary\n",
    "* **AGM belief revision**: rational rules for static correction\n",
    "* **Belief update**: dynamic adaptation to world changes\n",
    "* **Fusion and arbitration**: multi-source integration\n",
    "* **Inconsistency handling**: paraconsistency and minimal repair\n",
    "* **Temporal reasoning**: beliefs across time\n",
    "* **Intentions and commitments**: agent-level dynamics of goal pursuit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c1415f-038e-4381-bc60-b07f9d76f0cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Knowledge Representation ‚Äî Formal Ontologies**\n",
    "* An **ontology** is a formal, explicit specification of a shared conceptualization (Gruber, 1993).\n",
    "* Purpose:\n",
    "  * Define concepts (classes), relations, individuals.\n",
    "  * Enable **semantic interoperability** across systems.\n",
    "  * Support **reasoning**: subsumption, instance checking, consistency.\n",
    "\n",
    "**Examples:**\n",
    "* Medical ontology: $\\text{Diabetes} \\sqsubseteq \\text{Disease}$\n",
    "* Robotics ontology: $\\text{Robot} \\sqsubseteq \\text{Agent}$\n",
    "\n",
    "### Ontology Components\n",
    "An ontology $O$ can be modeled as a tuple: $O = (C, R, I, A)$\n",
    "\n",
    "where:\n",
    "* $C$ = set of concepts (classes)\n",
    "* $R$ = set of relations (roles, properties)\n",
    "* $I$ = set of individuals (instances)\n",
    "* $A$ = axioms (constraints on $C, R, I$)\n",
    "\n",
    "**Example:**\n",
    "* $C = {\\text{Human}, \\text{Animal}}$\n",
    "* $R = {\\text{hasParent}, \\text{eats}}$\n",
    "* $I = {\\text{Socrates}, \\text{Alice}}$\n",
    "* $A = {\\text{Human} \\sqsubseteq \\text{Animal}}$\n",
    "\n",
    "### Description Logics (DLs)\n",
    "Ontologies are formalized in **Description Logics (DLs)**.\n",
    "* Syntax constructs:\n",
    "  * $C \\sqcap D$ (intersection)\n",
    "  * $C \\sqcup D$ (union)\n",
    "  * $\\neg C$ (negation)\n",
    "  * $\\exists R.C$ (existential restriction)\n",
    "  * $\\forall R.C$ (universal restriction)\n",
    "* Semantics: interpretation $I=(\\Delta^I, \\cdot^I)$\n",
    "  * $C^I \\subseteq \\Delta^I$\n",
    "  * $R^I \\subseteq \\Delta^I \\times \\Delta^I$\n",
    "  * $a^I \\in \\Delta^I$\n",
    "\n",
    "**Example:**\n",
    "* $\\text{Bird} \\equiv \\text{Animal} \\sqcap \\exists \\text{hasCovering}.\\text{Feather}$\n",
    "\n",
    "\n",
    "### Ontology Reasoning Tasks\n",
    "Reasoners (e.g., Pellet, HermiT, FaCT++) support:\n",
    "* **Subsumption:** Does $C \\sqsubseteq D$ hold?\n",
    "* **Satisfiability:** Is $C \\not\\equiv \\bot$ (can $C$ have instances)?\n",
    "* **Instance checking:** Is $a \\in C$ entailed?\n",
    "* **Consistency:** Is ontology $O$ free of contradictions?\n",
    "\n",
    "**Example:**\n",
    "* Axioms: $\\text{Penguin} \\sqsubseteq \\text{Bird}$, $\\text{Bird} \\sqsubseteq \\text{Flies}$\n",
    "* Add $\\text{Penguin} \\sqsubseteq \\neg \\text{Flies}$\n",
    "* Reasoner detects inconsistency unless defaults/nonmonotonic rules applied.\n",
    "\n",
    "### Example: Traffic Ontology\n",
    "Concept hierarchy:\n",
    "* $\\text{Vehicle} \\sqsubseteq \\top$\n",
    "* $\\text{EmergencyVehicle} \\sqsubseteq \\text{Vehicle}$\n",
    "* $\\text{Ambulance} \\sqsubseteq \\text{EmergencyVehicle}$\n",
    "\n",
    "Properties:\n",
    "* $\\forall x (\\text{EmergencyVehicle}(x) \\rightarrow \\text{hasPriority}(x, \\text{High}))$\n",
    "\n",
    "Individual facts:\n",
    "* $\\text{Ambulance}(a_1)$\n",
    "\n",
    "**Entailment:**\n",
    "* From $\\text{Ambulance}(a_1)$, reasoner infers $\\text{EmergencyVehicle}(a_1)$ and $\\text{hasPriority}(a_1,\\text{High})$.\n",
    "\n",
    "### Ontology Languages\n",
    "* **OWL (Web Ontology Language):** W3C standard, built on DLs.\n",
    "  * OWL Lite, OWL DL, OWL Full.\n",
    "* **RDF/RDFS:** lightweight graph-based formalism.\n",
    "* **Common Logic, KIF, CycL:** expressive KR languages.\n",
    "\n",
    "**Example in OWL functional syntax:**\n",
    "```\n",
    "Class: Bird  \n",
    "  EquivalentTo: Animal and (hasCovering some Feather)\n",
    "```\n",
    "\n",
    "### Applications of Formal Ontologies\n",
    "* **Healthcare:** SNOMED CT, Gene Ontology.\n",
    "* **Robotics:** semantic maps for navigation.\n",
    "* **Semantic Web:** linked data, knowledge graphs.\n",
    "* **Multiagent Systems:** shared vocabularies enable cooperation.\n",
    "\n",
    "**Key role:** Ontologies bridge **data** and **reasoning** by making semantics explicit.\n",
    "\n",
    "\n",
    "### **Ontologies in Agentic AI**\n",
    "1. **Shared Vocabulary and Semantic Grounding**\n",
    "   * In a multiagent environment, agents need a **common language** to communicate about the world.\n",
    "   * Ontologies provide this by defining **concepts** (e.g., Vehicle, Road, TrafficLight) and **relations** (e.g., hasPriority, locatedAt).\n",
    "   * Example: two traffic-management agents can coordinate if they both recognize that *Ambulance ‚äë EmergencyVehicle ‚äë Vehicle* and that *EmergencyVehicles havePriority High*.\n",
    "2. **Reasoning and Inference for Autonomy**\n",
    "   * Agents can **reason over ontologies** to derive implicit knowledge.\n",
    "   * Example:\n",
    "     * Ontology axiom: $\\text{Ambulance} \\sqsubseteq \\text{EmergencyVehicle}$\n",
    "     * Rule: $\\forall x (\\text{EmergencyVehicle}(x) \\rightarrow \\text{hasPriority}(x,\\text{High}))$\n",
    "     * Fact: $\\text{Ambulance}(a_1)$\n",
    "     * Agent infers: $\\text{hasPriority}(a_1,\\text{High})$ without it being explicitly coded.\n",
    "3. **Coordination in Multiagent Systems**\n",
    "   * Agents may be designed by different teams or organizations.\n",
    "   * Ontologies support **semantic interoperability**, ensuring they can align on meaning even if their internal implementations differ.\n",
    "   * Example: Disaster-response drones from different vendors use a shared ontology of *resources*, *hazards*, and *tasks* to coordinate efficiently.\n",
    "4. **Integration with Learning Systems (Neuro-symbolic AI)**\n",
    "   * Agents often combine **data-driven perception** (neural networks) with **symbolic reasoning** (ontologies).\n",
    "   * Ontologies provide structure for **high-level decision-making**:\n",
    "     * NN classifier detects ‚Äúvehicle‚Äù in an image ‚Üí grounded to concept $\\text{Vehicle}$ in ontology.\n",
    "     * Ontology rules help agent decide: *if Vehicle ‚àß hasFlashingLights ‚Üí likely EmergencyVehicle*.\n",
    "   * This neuro-symbolic integration enables **explainability**, since decisions are traceable to ontology axioms.\n",
    "5. **Dynamic Ontology Use in Agentic AI**\n",
    "   * Agents may **extend or align ontologies at runtime** to adapt to new domains.\n",
    "   * Example:\n",
    "     * A supply-chain AI agent imports new product categories.\n",
    "     * Ontology alignment maps *‚Äúfragile-goods‚Äù* in one agent‚Äôs vocabulary to *‚Äúdelicate-items‚Äù* in another‚Äôs.\n",
    "   * This allows dynamic cooperation without manual intervention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118a127-ad52-4955-8c39-48eaad18f553",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "659dcf80-5cec-4525-a569-00901e00d82c",
   "metadata": {},
   "source": [
    "***\n",
    "## References\n",
    "1. Wooldridge, M. (2002). An Introduction to MultiAgent Systems. John Wiley & Sons. Ch. 12 (Logics for Multiagent Systems)\n",
    "2. Shoham, Y., & Leyton-Brown, K. (2009). Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. Cambridge University Press. Ch. 13‚Äì14\n",
    "3. Halpern, J. Y. (1995). Reasoning about Knowledge. MIT Press.\n",
    "4. Alchourr√≥n, C. E., G√§rdenfors, P., & Makinson, D. (1985). On the Logic of Theory Change: Partial Meet Contraction and Revision Functions. Journal of Symbolic Logic.\n",
    "\n",
    "***\n",
    "## Exercises\n",
    "__Exercise 1.__ \n",
    "\n",
    "__Exercise 2.__ \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3105c0d-0934-4e4d-ba22-c407fa1c3bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {margin-left: 0 !important;}\n",
       "    p {font-family: verdana;}\n",
       "    li {font-family: verdana;}\n",
       "    div {font-size: 10pt;}\n",
       "    ul {margin-top: 0 !important;}\n",
       "</style>\n",
       "<!-- Display markdown tables left oriented in this notebook. -->\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {margin-left: 0 !important;}\n",
    "    p {font-family: verdana;}\n",
    "    li {font-family: verdana;}\n",
    "    div {font-size: 10pt;}\n",
    "    ul {margin-top: 0 !important;}\n",
    "</style>\n",
    "<!-- Display markdown tables left oriented in this notebook. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3606b92-5057-43c3-95df-fdce19ae2668",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
